{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pruning Analysis\n",
    "\n",
    "## EE4745 Neural Networks Final Project\n",
    "\n",
    "This notebook demonstrates model pruning techniques for neural network compression and efficiency optimization.\n",
    "\n",
    "### Objectives:\n",
    "- Implement unstructured pruning techniques\n",
    "- Analyze pruning effects on model performance\n",
    "- Visualize pruning impact on network structure\n",
    "- Evaluate robustness of pruned models\n",
    "- Study the trade-offs between compression and accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import libraries and set up the environment for model pruning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from dataset.sports_dataset import SportsDataset, get_dataloaders\n",
    "from models.simple_cnn import create_simple_cnn\n",
    "from models.resnet_small import create_resnet_small\n",
    "from training.utils import set_seed, get_device, load_checkpoint, count_parameters\n",
    "from pruning.unstructured import prune_model, evaluate_pruned_model\n",
    "from attacks.fgsm import FGSM\n",
    "from attacks.pgd import PGD\n",
    "\n",
    "# Set style and configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Model Pruning Analysis Setup\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Pre-trained Models\n",
    "\n",
    "Load dataset and pre-trained models for pruning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = '../data'\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets for pruning analysis...\")\n",
    "train_loader, val_loader, num_classes = get_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "class_names = SportsDataset.CLASSES\n",
    "print(f\"Dataset loaded: {num_classes} classes\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "\n",
    "# Create a smaller subset for faster pruning experiments\n",
    "subset_size = 200\n",
    "subset_indices = torch.randperm(len(val_loader.dataset))[:subset_size]\n",
    "subset_dataset = torch.utils.data.Subset(val_loader.dataset, subset_indices)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Created subset with {subset_size} samples for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Models for Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_model(model_type, checkpoint_path=None):\n",
    "    \"\"\"Load a pre-trained model or create a new one\"\"\"\n",
    "    \n",
    "    if model_type == 'SimpleCNN':\n",
    "        model = create_simple_cnn(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "    elif model_type == 'ResNetSmall':\n",
    "        model = create_resnet_small(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        try:\n",
    "            load_checkpoint(checkpoint_path, model)\n",
    "            print(\"✅ Checkpoint loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to load checkpoint: {e}\")\n",
    "            print(\"Using randomly initialized model\")\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoint found, using randomly initialized {model_type}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def evaluate_model_accuracy(model, dataloader):\n",
    "    \"\"\"Evaluate model accuracy on a dataloader\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Define possible checkpoint paths\n",
    "checkpoint_paths = {\n",
    "    'SimpleCNN': '../checkpoints/SimpleCNN-best.pt',\n",
    "    'ResNetSmall': '../checkpoints/ResNetSmall-best.pt'\n",
    "}\n",
    "\n",
    "# Load models\n",
    "print(\"\\nLoading models for pruning...\")\n",
    "models = {}\n",
    "\n",
    "for model_type in ['SimpleCNN', 'ResNetSmall']:\n",
    "    print(f\"\\nLoading {model_type}...\")\n",
    "    models[model_type] = load_or_create_model(model_type, checkpoint_paths.get(model_type))\n",
    "    \n",
    "    # Evaluate baseline accuracy\n",
    "    baseline_accuracy = evaluate_model_accuracy(models[model_type], subset_loader)\n",
    "    print(f\"  Baseline accuracy: {baseline_accuracy:.2f}%\")\n",
    "    \n",
    "    # Model size information\n",
    "    total_params, trainable_params = count_parameters(models[model_type])\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Model size: {total_params * 4 / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\nModels loaded: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Structure Analysis\n",
    "\n",
    "Analyze the structure of models before pruning to understand pruning opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_structure(model, model_name):\n",
    "    \"\"\"Analyze model structure and identify pruneable layers\"\"\"\n",
    "    \n",
    "    print(f\"\\nAnalyzing {model_name} structure...\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    layer_info = []\n",
    "    total_params = 0\n",
    "    pruneable_params = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            if hasattr(module, 'weight'):\n",
    "                weight_params = module.weight.numel()\n",
    "                bias_params = module.bias.numel() if module.bias is not None else 0\n",
    "                layer_params = weight_params + bias_params\n",
    "                \n",
    "                layer_info.append({\n",
    "                    'name': name,\n",
    "                    'type': type(module).__name__,\n",
    "                    'shape': list(module.weight.shape),\n",
    "                    'weight_params': weight_params,\n",
    "                    'bias_params': bias_params,\n",
    "                    'total_params': layer_params,\n",
    "                    'pruneable': True\n",
    "                })\n",
    "                \n",
    "                total_params += layer_params\n",
    "                pruneable_params += weight_params  # Only weights are typically pruned\n",
    "                \n",
    "                print(f\"{name:30} | {type(module).__name__:10} | \"\n",
    "                      f\"Shape: {str(list(module.weight.shape)):20} | \"\n",
    "                      f\"Params: {layer_params:8,}\")\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total parameters in pruneable layers: {total_params:,}\")\n",
    "    print(f\"  Pruneable weight parameters: {pruneable_params:,}\")\n",
    "    print(f\"  Pruning potential: {pruneable_params/total_params*100:.1f}% of layer params\")\n",
    "    \n",
    "    return layer_info\n",
    "\n",
    "# Analyze structure of each model\n",
    "model_structures = {}\n",
    "for model_name, model in models.items():\n",
    "    model_structures[model_name] = analyze_model_structure(model, model_name)\n",
    "\n",
    "# Visualize model complexity\n",
    "def visualize_model_complexity(model_structures):\n",
    "    \"\"\"Visualize parameter distribution across layers\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(model_structures), figsize=(6*len(model_structures), 6))\n",
    "    \n",
    "    if len(model_structures) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, structure) in enumerate(model_structures.items()):\n",
    "        layer_names = [info['name'] for info in structure]\n",
    "        param_counts = [info['total_params'] for info in structure]\n",
    "        \n",
    "        # Create pie chart of parameter distribution\n",
    "        axes[idx].pie(param_counts, labels=layer_names, autopct='%1.1f%%', startangle=90)\n",
    "        axes[idx].set_title(f'{model_name} - Parameter Distribution', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for model_name, structure in model_structures.items():\n",
    "        total_params = sum(info['total_params'] for info in structure)\n",
    "        conv_params = sum(info['total_params'] for info in structure if info['type'] == 'Conv2d')\n",
    "        linear_params = sum(info['total_params'] for info in structure if info['type'] == 'Linear')\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Total Params': f\"{total_params:,}\",\n",
    "            'Conv Params': f\"{conv_params:,}\",\n",
    "            'Linear Params': f\"{linear_params:,}\",\n",
    "            'Conv %': f\"{conv_params/total_params*100:.1f}%\",\n",
    "            'Linear %': f\"{linear_params/total_params*100:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nModel Complexity Comparison:\")\n",
    "    print(comparison_df.to_string(index=False))\n",
    "\n",
    "visualize_model_complexity(model_structures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unstructured Pruning Implementation\n",
    "\n",
    "Implement magnitude-based unstructured pruning and analyze its effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_magnitude_pruning(model, pruning_ratio, layer_types=(nn.Conv2d, nn.Linear)):\n",
    "    \"\"\"\n",
    "    Apply magnitude-based unstructured pruning to a model\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to prune\n",
    "        pruning_ratio: Fraction of weights to prune (0.0 to 1.0)\n",
    "        layer_types: Types of layers to prune\n",
    "    \n",
    "    Returns:\n",
    "        pruned_model: Model with pruning applied\n",
    "        pruning_stats: Statistics about the pruning operation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the model to avoid modifying the original\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Collect all layers to prune\n",
    "    modules_to_prune = []\n",
    "    \n",
    "    for name, module in pruned_model.named_modules():\n",
    "        if isinstance(module, layer_types):\n",
    "            modules_to_prune.append((module, 'weight'))\n",
    "    \n",
    "    # Apply global magnitude pruning\n",
    "    prune.global_unstructured(\n",
    "        modules_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=pruning_ratio\n",
    "    )\n",
    "    \n",
    "    # Calculate pruning statistics\n",
    "    total_params = 0\n",
    "    pruned_params = 0\n",
    "    layer_stats = []\n",
    "    \n",
    "    for name, module in pruned_model.named_modules():\n",
    "        if isinstance(module, layer_types) and hasattr(module, 'weight_mask'):\n",
    "            mask = module.weight_mask\n",
    "            total_weights = mask.numel()\n",
    "            remaining_weights = mask.sum().item()\n",
    "            pruned_weights = total_weights - remaining_weights\n",
    "            \n",
    "            layer_stats.append({\n",
    "                'name': name,\n",
    "                'type': type(module).__name__,\n",
    "                'total_weights': total_weights,\n",
    "                'pruned_weights': pruned_weights,\n",
    "                'remaining_weights': remaining_weights,\n",
    "                'pruning_ratio': pruned_weights / total_weights,\n",
    "                'sparsity': pruned_weights / total_weights\n",
    "            })\n",
    "            \n",
    "            total_params += total_weights\n",
    "            pruned_params += pruned_weights\n",
    "    \n",
    "    overall_sparsity = pruned_params / total_params if total_params > 0 else 0\n",
    "    \n",
    "    pruning_stats = {\n",
    "        'target_ratio': pruning_ratio,\n",
    "        'actual_sparsity': overall_sparsity,\n",
    "        'total_params': total_params,\n",
    "        'pruned_params': pruned_params,\n",
    "        'remaining_params': total_params - pruned_params,\n",
    "        'layer_stats': layer_stats\n",
    "    }\n",
    "    \n",
    "    return pruned_model, pruning_stats\n",
    "\n",
    "def make_pruning_permanent(model):\n",
    "    \"\"\"Make pruning permanent by removing the masks and zeroed weights\"\"\"\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "def evaluate_pruning_impact(original_model, pruned_model, dataloader, model_name, pruning_ratio):\n",
    "    \"\"\"Evaluate the impact of pruning on model performance\"\"\"\n",
    "    \n",
    "    print(f\"\\nEvaluating pruning impact for {model_name} (ratio: {pruning_ratio:.1%})\")\n",
    "    \n",
    "    # Evaluate both models\n",
    "    original_acc = evaluate_model_accuracy(original_model, dataloader)\n",
    "    pruned_acc = evaluate_model_accuracy(pruned_model, dataloader)\n",
    "    \n",
    "    # Calculate model sizes\n",
    "    original_params = sum(p.numel() for p in original_model.parameters())\n",
    "    \n",
    "    # Count actual non-zero parameters in pruned model\n",
    "    pruned_nonzero = 0\n",
    "    for module in pruned_model.modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                pruned_nonzero += module.weight_mask.sum().item()\n",
    "                if hasattr(module, 'bias') and module.bias is not None:\n",
    "                    pruned_nonzero += module.bias.numel()\n",
    "            else:\n",
    "                pruned_nonzero += sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "    compression_ratio = original_params / pruned_nonzero if pruned_nonzero > 0 else float('inf')\n",
    "    accuracy_drop = original_acc - pruned_acc\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'pruning_ratio': pruning_ratio,\n",
    "        'original_accuracy': original_acc,\n",
    "        'pruned_accuracy': pruned_acc,\n",
    "        'accuracy_drop': accuracy_drop,\n",
    "        'original_params': original_params,\n",
    "        'pruned_nonzero_params': int(pruned_nonzero),\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'size_reduction': (1 - pruned_nonzero/original_params) * 100\n",
    "    }\n",
    "    \n",
    "    print(f\"  Original accuracy: {original_acc:.2f}%\")\n",
    "    print(f\"  Pruned accuracy: {pruned_acc:.2f}%\")\n",
    "    print(f\"  Accuracy drop: {accuracy_drop:.2f}%\")\n",
    "    print(f\"  Compression ratio: {compression_ratio:.2f}x\")\n",
    "    print(f\"  Size reduction: {results['size_reduction']:.1f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different pruning ratios\n",
    "pruning_ratios = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "pruning_results = []\n",
    "\n",
    "print(\"MAGNITUDE-BASED PRUNING ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name, original_model in models.items():\n",
    "    print(f\"\\nPruning {model_name}...\")\n",
    "    \n",
    "    for ratio in pruning_ratios:\n",
    "        # Apply pruning\n",
    "        pruned_model, pruning_stats = apply_magnitude_pruning(original_model, ratio)\n",
    "        \n",
    "        # Evaluate impact\n",
    "        result = evaluate_pruning_impact(\n",
    "            original_model, pruned_model, subset_loader, model_name, ratio\n",
    "        )\n",
    "        \n",
    "        # Add pruning statistics\n",
    "        result.update({\n",
    "            'actual_sparsity': pruning_stats['actual_sparsity'],\n",
    "            'target_sparsity': pruning_stats['target_ratio']\n",
    "        })\n",
    "        \n",
    "        pruning_results.append(result)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "pruning_df = pd.DataFrame(pruning_results)\n",
    "print(\"\\nPruning analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pruning Results Visualization\n",
    "\n",
    "Visualize the effects of pruning on model performance and compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pruning_results(pruning_df):\n",
    "    \"\"\"Visualize pruning results across different ratios and models\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Pruning Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models = pruning_df['model_name'].unique()\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    # Plot 1: Accuracy vs Pruning Ratio\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        \n",
    "        # Original accuracy line\n",
    "        orig_acc = model_data['original_accuracy'].iloc[0]\n",
    "        axes[0, 0].axhline(y=orig_acc, color=colors[i], linestyle='--', alpha=0.5, \n",
    "                          label=f'{model} Original')\n",
    "        \n",
    "        # Pruned accuracy\n",
    "        axes[0, 0].plot(model_data['pruning_ratio'], model_data['pruned_accuracy'], \n",
    "                       'o-', color=colors[i], label=f'{model} Pruned', linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[0, 0].set_title('Accuracy vs Pruning Ratio', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Pruning Ratio')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Compression Ratio vs Accuracy Drop\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        axes[0, 1].scatter(model_data['compression_ratio'], model_data['accuracy_drop'], \n",
    "                          s=100, color=colors[i], label=model, alpha=0.7)\n",
    "        \n",
    "        # Add pruning ratio annotations\n",
    "        for _, row in model_data.iterrows():\n",
    "            axes[0, 1].annotate(f'{row[\"pruning_ratio\"]:.1f}', \n",
    "                               (row['compression_ratio'], row['accuracy_drop']),\n",
    "                               xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    axes[0, 1].set_title('Compression vs Accuracy Trade-off', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Compression Ratio')\n",
    "    axes[0, 1].set_ylabel('Accuracy Drop (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Size Reduction vs Pruning Ratio\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        axes[1, 0].plot(model_data['pruning_ratio'], model_data['size_reduction'], \n",
    "                       's-', color=colors[i], label=model, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[1, 0].set_title('Model Size Reduction', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Pruning Ratio')\n",
    "    axes[1, 0].set_ylabel('Size Reduction (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Efficiency Score (Accuracy / Model Size)\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        \n",
    "        # Calculate efficiency score\n",
    "        efficiency_score = (model_data['pruned_accuracy'] / model_data['pruned_nonzero_params']) * 1000000\n",
    "        \n",
    "        axes[1, 1].plot(model_data['pruning_ratio'], efficiency_score, \n",
    "                       '^-', color=colors[i], label=model, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[1, 1].set_title('Model Efficiency Score\\n(Accuracy per Million Parameters)', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Pruning Ratio')\n",
    "    axes[1, 1].set_ylabel('Efficiency Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_pruning_summary_table(pruning_df):\n",
    "    \"\"\"Create a summary table of pruning results\"\"\"\n",
    "    \n",
    "    summary_data = []\n",
    "    \n",
    "    for model in pruning_df['model_name'].unique():\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        \n",
    "        # Find best pruning ratio (minimal accuracy drop with good compression)\n",
    "        # Use a composite score: compression_ratio / (1 + accuracy_drop)\n",
    "        model_data = model_data.copy()\n",
    "        model_data['composite_score'] = (model_data['compression_ratio'] / \n",
    "                                       (1 + model_data['accuracy_drop']))\n",
    "        \n",
    "        best_idx = model_data['composite_score'].idxmax()\n",
    "        best_result = model_data.loc[best_idx]\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': model,\n",
    "            'Original Acc (%)': f\"{best_result['original_accuracy']:.2f}\",\n",
    "            'Best Pruning Ratio': f\"{best_result['pruning_ratio']:.1f}\",\n",
    "            'Pruned Acc (%)': f\"{best_result['pruned_accuracy']:.2f}\",\n",
    "            'Accuracy Drop (%)': f\"{best_result['accuracy_drop']:.2f}\",\n",
    "            'Compression': f\"{best_result['compression_ratio']:.1f}x\",\n",
    "            'Size Reduction (%)': f\"{best_result['size_reduction']:.1f}\",\n",
    "            'Efficiency Score': f\"{(best_result['pruned_accuracy'] / best_result['pruned_nonzero_params']) * 1000000:.2f}\"\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nPruning Summary Table (Best Trade-off):\")\n",
    "    print(\"=\" * 60)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Visualize pruning results\n",
    "visualize_pruning_results(pruning_df)\n",
    "\n",
    "# Create summary table\n",
    "summary_table = create_pruning_summary_table(pruning_df)\n",
    "\n",
    "# Print detailed results for each model\n",
    "print(\"\\n\\nDetailed Pruning Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model in pruning_df['model_name'].unique():\n",
    "    model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"{'Ratio':>8} {'Accuracy':>10} {'Drop':>8} {'Compression':>12} {'Size Red.':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for _, row in model_data.iterrows():\n",
    "        print(f\"{row['pruning_ratio']:>8.1f} {row['pruned_accuracy']:>9.2f}% \"\n",
    "              f\"{row['accuracy_drop']:>7.2f}% {row['compression_ratio']:>10.1f}x \"\n",
    "              f\"{row['size_reduction']:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Layer-wise Pruning Analysis\n",
    "\n",
    "Analyze how pruning affects different layers of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layerwise_pruning(model, pruning_ratio=0.5):\n",
    "    \"\"\"Analyze how pruning affects different layers\"\"\"\n",
    "    \n",
    "    # Apply pruning to get detailed statistics\n",
    "    pruned_model, pruning_stats = apply_magnitude_pruning(model, pruning_ratio)\n",
    "    \n",
    "    layer_stats = pruning_stats['layer_stats']\n",
    "    \n",
    "    print(f\"\\nLayer-wise Pruning Analysis (ratio: {pruning_ratio:.1%})\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{'Layer':30} {'Type':10} {'Total':>10} {'Pruned':>10} {'Sparsity':>10}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for stat in layer_stats:\n",
    "        print(f\"{stat['name']:30} {stat['type']:10} {stat['total_weights']:>10,} \"\n",
    "              f\"{stat['pruned_weights']:>10,} {stat['sparsity']:>9.1%}\")\n",
    "    \n",
    "    return layer_stats, pruned_model\n",
    "\n",
    "def visualize_layerwise_sparsity(layer_stats_dict):\n",
    "    \"\"\"Visualize sparsity across different layers\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(layer_stats_dict), figsize=(6*len(layer_stats_dict), 6))\n",
    "    \n",
    "    if len(layer_stats_dict) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (model_name, layer_stats) in enumerate(layer_stats_dict.items()):\n",
    "        layer_names = [stat['name'].split('.')[-1] for stat in layer_stats]  # Short names\n",
    "        sparsities = [stat['sparsity'] * 100 for stat in layer_stats]\n",
    "        layer_types = [stat['type'] for stat in layer_stats]\n",
    "        \n",
    "        # Color by layer type\n",
    "        colors = ['skyblue' if t == 'Conv2d' else 'lightcoral' for t in layer_types]\n",
    "        \n",
    "        bars = axes[idx].bar(range(len(layer_names)), sparsities, color=colors, alpha=0.7)\n",
    "        axes[idx].set_title(f'{model_name} - Layer-wise Sparsity', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Layers')\n",
    "        axes[idx].set_ylabel('Sparsity (%)')\n",
    "        axes[idx].set_xticks(range(len(layer_names)))\n",
    "        axes[idx].set_xticklabels(layer_names, rotation=45, ha='right')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, sparsity in zip(bars, sparsities):\n",
    "            height = bar.get_height()\n",
    "            axes[idx].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                          f'{sparsity:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Create legend\n",
    "        conv_patch = plt.Rectangle((0,0), 1, 1, facecolor='skyblue', alpha=0.7, label='Conv2d')\n",
    "        linear_patch = plt.Rectangle((0,0), 1, 1, facecolor='lightcoral', alpha=0.7, label='Linear')\n",
    "        axes[idx].legend(handles=[conv_patch, linear_patch])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze layer-wise pruning for each model\n",
    "layer_stats_dict = {}\n",
    "pruned_models_50 = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nAnalyzing layer-wise pruning for {model_name}...\")\n",
    "    layer_stats, pruned_model = analyze_layerwise_pruning(model, pruning_ratio=0.5)\n",
    "    layer_stats_dict[model_name] = layer_stats\n",
    "    pruned_models_50[model_name] = pruned_model\n",
    "\n",
    "# Visualize layer-wise sparsity\n",
    "visualize_layerwise_sparsity(layer_stats_dict)\n",
    "\n",
    "# Analyze parameter distribution after pruning\n",
    "print(\"\\nParameter Distribution Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name, layer_stats in layer_stats_dict.items():\n",
    "    total_original = sum(stat['total_weights'] for stat in layer_stats)\n",
    "    total_remaining = sum(stat['remaining_weights'] for stat in layer_stats)\n",
    "    \n",
    "    conv_original = sum(stat['total_weights'] for stat in layer_stats if stat['type'] == 'Conv2d')\n",
    "    conv_remaining = sum(stat['remaining_weights'] for stat in layer_stats if stat['type'] == 'Conv2d')\n",
    "    \n",
    "    linear_original = sum(stat['total_weights'] for stat in layer_stats if stat['type'] == 'Linear')\n",
    "    linear_remaining = sum(stat['remaining_weights'] for stat in layer_stats if stat['type'] == 'Linear')\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Total parameters: {total_original:,} → {total_remaining:,} ({total_remaining/total_original:.1%})\")\n",
    "    print(f\"  Conv parameters: {conv_original:,} → {conv_remaining:,} ({conv_remaining/conv_original:.1%})\")\n",
    "    print(f\"  Linear parameters: {linear_original:,} → {linear_remaining:,} ({linear_remaining/linear_original:.1%})\")\n",
    "\n",
    "print(\"\\nLayer-wise analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Robustness Analysis of Pruned Models\n",
    "\n",
    "Analyze how pruning affects model robustness to adversarial attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adversarial_robustness(model, dataloader, model_name, attack_configs):\n",
    "    \"\"\"Evaluate model robustness against adversarial attacks\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Clean accuracy\n",
    "    clean_acc = evaluate_model_accuracy(model, dataloader)\n",
    "    results['clean'] = clean_acc\n",
    "    \n",
    "    print(f\"\\nEvaluating adversarial robustness for {model_name}...\")\n",
    "    print(f\"Clean accuracy: {clean_acc:.2f}%\")\n",
    "    \n",
    "    for attack_name, config in attack_configs.items():\n",
    "        if config['type'] == 'fgsm':\n",
    "            attack = FGSM(model, device=device)\n",
    "        elif config['type'] == 'pgd':\n",
    "            attack = PGD(model, device=device, steps=config.get('steps', 10), \n",
    "                        step_size=config.get('step_size', 0.01))\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate attack\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Generate adversarial examples\n",
    "            adv_images = attack.attack(images, labels, epsilon=config['epsilon'])\n",
    "            \n",
    "            # Test on adversarial examples\n",
    "            with torch.no_grad():\n",
    "                outputs = model(adv_images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        adv_acc = 100.0 * correct / total\n",
    "        results[attack_name] = adv_acc\n",
    "        \n",
    "        print(f\"{attack_name} accuracy (ε={config['epsilon']}): {adv_acc:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compare_robustness_before_after_pruning(original_models, pruned_models, dataloader):\n",
    "    \"\"\"Compare robustness before and after pruning\"\"\"\n",
    "    \n",
    "    # Attack configurations to test\n",
    "    attack_configs = {\n",
    "        'FGSM_mild': {'type': 'fgsm', 'epsilon': 0.05},\n",
    "        'FGSM_strong': {'type': 'fgsm', 'epsilon': 0.1},\n",
    "        'PGD_mild': {'type': 'pgd', 'epsilon': 0.05, 'steps': 10, 'step_size': 0.005},\n",
    "        'PGD_strong': {'type': 'pgd', 'epsilon': 0.1, 'steps': 10, 'step_size': 0.01}\n",
    "    }\n",
    "    \n",
    "    robustness_comparison = []\n",
    "    \n",
    "    for model_name in original_models.keys():\n",
    "        if model_name in pruned_models:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"ROBUSTNESS COMPARISON: {model_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Evaluate original model\n",
    "            original_results = evaluate_adversarial_robustness(\n",
    "                original_models[model_name], dataloader, f\"{model_name}_original\", attack_configs\n",
    "            )\n",
    "            \n",
    "            # Evaluate pruned model\n",
    "            pruned_results = evaluate_adversarial_robustness(\n",
    "                pruned_models[model_name], dataloader, f\"{model_name}_pruned\", attack_configs\n",
    "            )\n",
    "            \n",
    "            # Calculate changes\n",
    "            for attack_name in attack_configs.keys():\n",
    "                original_acc = original_results.get(attack_name, 0)\n",
    "                pruned_acc = pruned_results.get(attack_name, 0)\n",
    "                change = pruned_acc - original_acc\n",
    "                \n",
    "                robustness_comparison.append({\n",
    "                    'model': model_name,\n",
    "                    'attack': attack_name,\n",
    "                    'original_acc': original_acc,\n",
    "                    'pruned_acc': pruned_acc,\n",
    "                    'change': change\n",
    "                })\n",
    "    \n",
    "    return robustness_comparison\n",
    "\n",
    "def visualize_robustness_comparison(robustness_data):\n",
    "    \"\"\"Visualize robustness comparison before and after pruning\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(robustness_data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No robustness data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    models = df['model'].unique()\n",
    "    attacks = df['attack'].unique()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Accuracy comparison\n",
    "    x = np.arange(len(attacks))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        if not model_data.empty:\n",
    "            original_accs = [model_data[model_data['attack'] == attack]['original_acc'].iloc[0] \n",
    "                           for attack in attacks if not model_data[model_data['attack'] == attack].empty]\n",
    "            pruned_accs = [model_data[model_data['attack'] == attack]['pruned_acc'].iloc[0] \n",
    "                         for attack in attacks if not model_data[model_data['attack'] == attack].empty]\n",
    "            \n",
    "            if len(original_accs) == len(attacks):\n",
    "                offset = (i - len(models)/2 + 0.5) * width / len(models)\n",
    "                axes[0].bar(x + offset - width/4, original_accs, width/2, \n",
    "                           label=f'{model} Original', alpha=0.8)\n",
    "                axes[0].bar(x + offset + width/4, pruned_accs, width/2, \n",
    "                           label=f'{model} Pruned', alpha=0.8)\n",
    "    \n",
    "    axes[0].set_title('Adversarial Robustness: Original vs Pruned', fontweight='bold')\n",
    "    axes[0].set_xlabel('Attack Type')\n",
    "    axes[0].set_ylabel('Accuracy (%)')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(attacks, rotation=45, ha='right')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Change in robustness\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        if not model_data.empty:\n",
    "            changes = [model_data[model_data['attack'] == attack]['change'].iloc[0] \n",
    "                      for attack in attacks if not model_data[model_data['attack'] == attack].empty]\n",
    "            \n",
    "            if len(changes) == len(attacks):\n",
    "                offset = (i - len(models)/2 + 0.5) * width / len(models)\n",
    "                bars = axes[1].bar(x + offset, changes, width/len(models), \n",
    "                                  label=model, alpha=0.8)\n",
    "                \n",
    "                # Color bars based on positive/negative change\n",
    "                for bar, change in zip(bars, changes):\n",
    "                    if change < 0:\n",
    "                        bar.set_color('red')\n",
    "                    else:\n",
    "                        bar.set_color('green')\n",
    "    \n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[1].set_title('Change in Robustness After Pruning', fontweight='bold')\n",
    "    axes[1].set_xlabel('Attack Type')\n",
    "    axes[1].set_ylabel('Accuracy Change (%)')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(attacks, rotation=45, ha='right')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare robustness before and after pruning\n",
    "print(\"ROBUSTNESS ANALYSIS OF PRUNED MODELS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Use the 50% pruned models from the previous section\n",
    "if pruned_models_50:\n",
    "    robustness_comparison = compare_robustness_before_after_pruning(\n",
    "        models, pruned_models_50, subset_loader\n",
    "    )\n",
    "    \n",
    "    # Visualize robustness comparison\n",
    "    visualize_robustness_comparison(robustness_comparison)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nRobustness Analysis Summary:\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        if model_name in pruned_models_50:\n",
    "            model_data = [r for r in robustness_comparison if r['model'] == model_name]\n",
    "            \n",
    "            if model_data:\n",
    "                avg_change = np.mean([r['change'] for r in model_data])\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"  Average robustness change: {avg_change:+.2f}%\")\n",
    "                \n",
    "                if avg_change > 1:\n",
    "                    print(f\"  ✅ Improved robustness after pruning\")\n",
    "                elif avg_change > -2:\n",
    "                    print(f\"  ⚠️  Minimal impact on robustness\")\n",
    "                else:\n",
    "                    print(f\"  ❌ Decreased robustness after pruning\")\n",
    "                \n",
    "                for r in model_data:\n",
    "                    print(f\"    {r['attack']}: {r['change']:+.2f}%\")\n",
    "\nelse:\n",
    "    print(\"No pruned models available for robustness analysis.\")\n",
    "\n",
    "print(\"\\nRobustness analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Pruning Techniques\n",
    "\n",
    "Explore structured pruning and gradual pruning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_pruning(model, pruning_ratio):\n",
    "    \"\"\"Apply structured pruning by removing entire channels\"\"\"\n",
    "    \n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    \n",
    "    # For demonstration, we'll apply channel-wise pruning to convolutional layers\n",
    "    conv_modules = [(name, module) for name, module in pruned_model.named_modules() \n",
    "                   if isinstance(module, nn.Conv2d)]\n",
    "    \n",
    "    pruning_stats = []\n",
    "    \n",
    "    for name, module in conv_modules:\n",
    "        if module.weight.size(0) > 1:  # Don't prune if only 1 channel\n",
    "            # Calculate channel importance (L1 norm)\n",
    "            channel_importance = torch.sum(torch.abs(module.weight.view(module.weight.size(0), -1)), dim=1)\n",
    "            \n",
    "            # Determine number of channels to prune\n",
    "            num_channels = module.weight.size(0)\n",
    "            num_to_prune = int(num_channels * pruning_ratio)\n",
    "            \n",
    "            if num_to_prune > 0 and num_to_prune < num_channels:\n",
    "                # Get indices of least important channels\n",
    "                _, indices_to_prune = torch.topk(channel_importance, num_to_prune, largest=False)\n",
    "                \n",
    "                # Create mask for remaining channels\n",
    "                channel_mask = torch.ones(num_channels, dtype=torch.bool)\n",
    "                channel_mask[indices_to_prune] = False\n",
    "                \n",
    "                pruning_stats.append({\n",
    "                    'layer': name,\n",
    "                    'original_channels': num_channels,\n",
    "                    'pruned_channels': num_to_prune,\n",
    "                    'remaining_channels': num_channels - num_to_prune\n",
    "                })\n",
    "    \n",
    "    return pruned_model, pruning_stats\n",
    "\n",
    "def gradual_pruning_simulation(model, dataloader, target_ratio=0.8, steps=5):\n",
    "    \"\"\"Simulate gradual pruning process\"\"\"\n",
    "    \n",
    "    current_model = copy.deepcopy(model)\n",
    "    gradual_results = []\n",
    "    \n",
    "    # Calculate pruning ratio per step\n",
    "    step_ratio = target_ratio / steps\n",
    "    \n",
    "    print(f\"\\nGradual Pruning Simulation (target: {target_ratio:.1%}, steps: {steps})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initial evaluation\n",
    "    initial_acc = evaluate_model_accuracy(current_model, dataloader)\n",
    "    initial_params = sum(p.numel() for p in current_model.parameters())\n",
    "    \n",
    "    gradual_results.append({\n",
    "        'step': 0,\n",
    "        'pruning_ratio': 0.0,\n",
    "        'cumulative_ratio': 0.0,\n",
    "        'accuracy': initial_acc,\n",
    "        'remaining_params': initial_params,\n",
    "        'compression_ratio': 1.0\n",
    "    })\n",
    "    \n",
    "    print(f\"Step 0 (Initial): Acc={initial_acc:.2f}%, Params={initial_params:,}\")\n",
    "    \n",
    "    cumulative_pruned = 0.0\n",
    "    \n",
    "    for step in range(1, steps + 1):\n",
    "        # Apply pruning for this step\n",
    "        current_model, _ = apply_magnitude_pruning(current_model, step_ratio)\n",
    "        \n",
    "        # Make pruning permanent to get actual parameter reduction\n",
    "        temp_model = copy.deepcopy(current_model)\n",
    "        make_pruning_permanent(temp_model)\n",
    "        \n",
    "        # Evaluate\n",
    "        step_acc = evaluate_model_accuracy(current_model, dataloader)\n",
    "        \n",
    "        # Count remaining parameters\n",
    "        remaining_params = 0\n",
    "        for module in current_model.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                if hasattr(module, 'weight_mask'):\n",
    "                    remaining_params += module.weight_mask.sum().item()\n",
    "                    if hasattr(module, 'bias') and module.bias is not None:\n",
    "                        remaining_params += module.bias.numel()\n",
    "                else:\n",
    "                    remaining_params += sum(p.numel() for p in module.parameters())\n",
    "        \n",
    "        cumulative_pruned = 1 - (remaining_params / initial_params)\n",
    "        compression_ratio = initial_params / remaining_params\n",
    "        \n",
    "        gradual_results.append({\n",
    "            'step': step,\n",
    "            'pruning_ratio': step_ratio,\n",
    "            'cumulative_ratio': cumulative_pruned,\n",
    "            'accuracy': step_acc,\n",
    "            'remaining_params': int(remaining_params),\n",
    "            'compression_ratio': compression_ratio\n",
    "        })\n",
    "        \n",
    "        print(f\"Step {step}: Acc={step_acc:.2f}%, \"\n",
    "              f\"Cumulative pruning={cumulative_pruned:.1%}, \"\n",
    "              f\"Compression={compression_ratio:.1f}x\")\n",
    "    \n",
    "    return gradual_results\n",
    "\n",
    "def visualize_advanced_pruning(gradual_results, structured_results=None):\n",
    "    \"\"\"Visualize results from advanced pruning techniques\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Gradual pruning progression\n",
    "    steps = [r['step'] for r in gradual_results]\n",
    "    accuracies = [r['accuracy'] for r in gradual_results]\n",
    "    cumulative_ratios = [r['cumulative_ratio'] * 100 for r in gradual_results]\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    color1 = 'tab:blue'\n",
    "    ax1.set_xlabel('Pruning Step')\n",
    "    ax1.set_ylabel('Accuracy (%)', color=color1)\n",
    "    line1 = ax1.plot(steps, accuracies, 'o-', color=color1, label='Accuracy', linewidth=2, markersize=6)\n",
    "    ax1.tick_params(axis='y', labelcolor=color1)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    color2 = 'tab:red'\n",
    "    ax2.set_ylabel('Cumulative Pruning (%)', color=color2)\n",
    "    line2 = ax2.plot(steps, cumulative_ratios, 's-', color=color2, label='Cumulative Pruning', \n",
    "                    linewidth=2, markersize=6)\n",
    "    ax2.tick_params(axis='y', labelcolor=color2)\n",
    "    \n",
    "    # Add legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc='center right')\n",
    "    \n",
    "    axes[0].set_title('Gradual Pruning Progression', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Comparison of different pruning ratios\n",
    "    if len(gradual_results) > 1:\n",
    "        compression_ratios = [r['compression_ratio'] for r in gradual_results]\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = axes[1].scatter(compression_ratios, accuracies, \n",
    "                                c=steps, cmap='viridis', s=100, alpha=0.7)\n",
    "        \n",
    "        # Add step labels\n",
    "        for i, (comp, acc, step) in enumerate(zip(compression_ratios, accuracies, steps)):\n",
    "            axes[1].annotate(f'Step {step}', (comp, acc), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[1].set_xlabel('Compression Ratio')\n",
    "        axes[1].set_ylabel('Accuracy (%)')\n",
    "        axes[1].set_title('Accuracy vs Compression Trade-off', fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "        cbar.set_label('Pruning Step')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test advanced pruning techniques\n",
    "print(\"ADVANCED PRUNING TECHNIQUES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Select a model for advanced pruning\n",
    "if models:\n",
    "    test_model_name = list(models.keys())[0]  # Use first available model\n",
    "    test_model = models[test_model_name]\n",
    "    \n",
    "    print(f\"Testing advanced pruning on {test_model_name}...\")\n",
    "    \n",
    "    # 1. Structured pruning (conceptual - actual implementation would be more complex)\n",
    "    print(f\"\\n1. Structured Pruning Analysis:\")\n",
    "    structured_model, structured_stats = structured_pruning(test_model, 0.3)\n",
    "    \n",
    "    if structured_stats:\n",
    "        print(\"\\nStructured Pruning Results:\")\n",
    "        for stat in structured_stats:\n",
    "            print(f\"  {stat['layer']}: {stat['original_channels']} → \"\n",
    "                  f\"{stat['remaining_channels']} channels \"\n",
    "                  f\"({stat['pruned_channels']} pruned)\")\n",
    "    \n",
    "    # 2. Gradual pruning simulation\n",
    "    print(f\"\\n2. Gradual Pruning Simulation:\")\n",
    "    gradual_results = gradual_pruning_simulation(test_model, subset_loader, \n",
    "                                                target_ratio=0.7, steps=4)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_advanced_pruning(gradual_results)\n",
    "    \n",
    "    # Analysis of gradual vs one-shot pruning\n",
    "    print(\"\\nGradual vs One-Shot Pruning Comparison:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    final_gradual = gradual_results[-1]\n",
    "    \n",
    "    # Apply one-shot pruning with same target ratio\n",
    "    target_ratio = final_gradual['cumulative_ratio']\n",
    "    oneshot_model, _ = apply_magnitude_pruning(test_model, target_ratio)\n",
    "    oneshot_acc = evaluate_model_accuracy(oneshot_model, subset_loader)\n",
    "    \n",
    "    print(f\"Target pruning ratio: {target_ratio:.1%}\")\n",
    "    print(f\"Gradual pruning final accuracy: {final_gradual['accuracy']:.2f}%\")\n",
    "    print(f\"One-shot pruning accuracy: {oneshot_acc:.2f}%\")\n",
    "    print(f\"Gradual advantage: {final_gradual['accuracy'] - oneshot_acc:+.2f}%\")\n",
    "    \n",
    "    if final_gradual['accuracy'] > oneshot_acc:\n",
    "        print(\"✅ Gradual pruning shows better accuracy retention\")\n",
    "    else:\n",
    "        print(\"⚠️  One-shot pruning performed similarly or better\")\n",
    "\nelse:\n",
    "    print(\"No models available for advanced pruning analysis.\")\n",
    "\n",
    "print(\"\\nAdvanced pruning analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Speed and Memory Analysis\n",
    "\n",
    "Analyze the practical benefits of pruning in terms of inference speed and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_speed(model, input_size=(1, 3, 32, 32), num_trials=100, warmup_trials=10):\n",
    "    \"\"\"Measure model inference speed\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(*input_size).to(device)\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_trials):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    # Actual timing\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_trials):\n",
    "            _ = model(dummy_input)\n",
    "    \n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time_ms = (end_time - start_time) * 1000 / num_trials\n",
    "    return avg_time_ms\n",
    "\n",
    "def measure_memory_usage(model, input_size=(1, 3, 32, 32)):\n",
    "    \"\"\"Measure model memory usage\"\"\"\n",
    "    \n",
    "    # Model parameter memory\n",
    "    param_memory_mb = sum(p.numel() for p in model.parameters()) * 4 / (1024**2)  # 4 bytes per float32\n",
    "    \n",
    "    # Activation memory (approximate)\n",
    "    dummy_input = torch.randn(*input_size).to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "        \n",
    "        peak_memory_mb = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "        activation_memory_mb = peak_memory_mb - param_memory_mb\n",
    "    else:\n",
    "        # Rough estimation for CPU\n",
    "        activation_memory_mb = param_memory_mb * 0.5  # Very rough estimate\n",
    "        peak_memory_mb = param_memory_mb + activation_memory_mb\n",
    "    \n",
    "    return {\n",
    "        'param_memory_mb': param_memory_mb,\n",
    "        'activation_memory_mb': max(0, activation_memory_mb),\n",
    "        'total_memory_mb': peak_memory_mb\n",
    "    }\n",
    "\n",
    "def comprehensive_performance_analysis(original_models, pruned_models_dict):\n",
    "    \"\"\"Comprehensive analysis of performance improvements from pruning\"\"\"\n",
    "    \n",
    "    performance_data = []\n",
    "    \n",
    "    print(\"\\nCOMPREHENSIVE PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    for model_name, original_model in original_models.items():\n",
    "        print(f\"\\nAnalyzing {model_name}...\")\n",
    "        \n",
    "        # Original model analysis\n",
    "        original_speed = measure_inference_speed(original_model)\n",
    "        original_memory = measure_memory_usage(original_model)\n",
    "        original_params = sum(p.numel() for p in original_model.parameters())\n",
    "        original_acc = evaluate_model_accuracy(original_model, subset_loader)\n",
    "        \n",
    "        performance_data.append({\n",
    "            'model': model_name,\n",
    "            'variant': 'Original',\n",
    "            'pruning_ratio': 0.0,\n",
    "            'accuracy': original_acc,\n",
    "            'inference_time_ms': original_speed,\n",
    "            'param_memory_mb': original_memory['param_memory_mb'],\n",
    "            'total_memory_mb': original_memory['total_memory_mb'],\n",
    "            'parameters': original_params,\n",
    "            'speedup': 1.0,\n",
    "            'memory_reduction': 0.0\n",
    "        })\n",
    "        \n",
    "        print(f\"  Original - Acc: {original_acc:.2f}%, \"\n",
    "              f\"Speed: {original_speed:.2f}ms, \"\n",
    "              f\"Memory: {original_memory['total_memory_mb']:.1f}MB\")\n",
    "        \n",
    "        # Analyze pruned variants if available\n",
    "        if model_name in pruned_models_dict:\n",
    "            pruned_model = pruned_models_dict[model_name]\n",
    "            \n",
    "            pruned_speed = measure_inference_speed(pruned_model)\n",
    "            pruned_memory = measure_memory_usage(pruned_model)\n",
    "            pruned_acc = evaluate_model_accuracy(pruned_model, subset_loader)\n",
    "            \n",
    "            # Count effective parameters (non-masked)\n",
    "            pruned_params = 0\n",
    "            for module in pruned_model.modules():\n",
    "                if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                    if hasattr(module, 'weight_mask'):\n",
    "                        pruned_params += module.weight_mask.sum().item()\n",
    "                        if hasattr(module, 'bias') and module.bias is not None:\n",
    "                            pruned_params += module.bias.numel()\n",
    "                    else:\n",
    "                        pruned_params += sum(p.numel() for p in module.parameters())\n",
    "            \n",
    "            speedup = original_speed / pruned_speed if pruned_speed > 0 else 1.0\n",
    "            memory_reduction = ((original_memory['total_memory_mb'] - pruned_memory['total_memory_mb']) / \n",
    "                              original_memory['total_memory_mb']) * 100\n",
    "            \n",
    "            performance_data.append({\n",
    "                'model': model_name,\n",
    "                'variant': 'Pruned (50%)',\n",
    "                'pruning_ratio': 0.5,\n",
    "                'accuracy': pruned_acc,\n",
    "                'inference_time_ms': pruned_speed,\n",
    "                'param_memory_mb': pruned_memory['param_memory_mb'],\n",
    "                'total_memory_mb': pruned_memory['total_memory_mb'],\n",
    "                'parameters': int(pruned_params),\n",
    "                'speedup': speedup,\n",
    "                'memory_reduction': memory_reduction\n",
    "            })\n",
    "            \n",
    "            print(f\"  Pruned   - Acc: {pruned_acc:.2f}%, \"\n",
    "                  f\"Speed: {pruned_speed:.2f}ms ({speedup:.1f}x speedup), \"\n",
    "                  f\"Memory: {pruned_memory['total_memory_mb']:.1f}MB ({memory_reduction:.1f}% reduction)\")\n",
    "    \n",
    "    return performance_data\n",
    "\n",
    "def visualize_performance_analysis(performance_data):\n",
    "    \"\"\"Visualize performance analysis results\"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No performance data to visualize.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Performance Analysis: Original vs Pruned Models', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models = df['model'].unique()\n",
    "    \n",
    "    # Plot 1: Accuracy vs Inference Time\n",
    "    for model in models:\n",
    "        model_data = df[df['model'] == model]\n",
    "        \n",
    "        original_data = model_data[model_data['variant'] == 'Original']\n",
    "        pruned_data = model_data[model_data['variant'] != 'Original']\n",
    "        \n",
    "        if not original_data.empty:\n",
    "            axes[0, 0].scatter(original_data['inference_time_ms'], original_data['accuracy'], \n",
    "                             s=150, alpha=0.7, marker='o', label=f'{model} Original')\n",
    "        \n",
    "        if not pruned_data.empty:\n",
    "            axes[0, 0].scatter(pruned_data['inference_time_ms'], pruned_data['accuracy'], \n",
    "                             s=150, alpha=0.7, marker='s', label=f'{model} Pruned')\n",
    "    \n",
    "    axes[0, 0].set_title('Accuracy vs Inference Time', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Inference Time (ms)')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Memory Usage Comparison\n",
    "    if len(models) > 0:\n",
    "        x_pos = np.arange(len(models))\n",
    "        width = 0.35\n",
    "        \n",
    "        original_memory = []\n",
    "        pruned_memory = []\n",
    "        \n",
    "        for model in models:\n",
    "            model_data = df[df['model'] == model]\n",
    "            \n",
    "            orig_data = model_data[model_data['variant'] == 'Original']\n",
    "            prun_data = model_data[model_data['variant'] != 'Original']\n",
    "            \n",
    "            original_memory.append(orig_data['total_memory_mb'].iloc[0] if not orig_data.empty else 0)\n",
    "            pruned_memory.append(prun_data['total_memory_mb'].iloc[0] if not prun_data.empty else 0)\n",
    "        \n",
    "        axes[0, 1].bar(x_pos - width/2, original_memory, width, label='Original', alpha=0.8)\n",
    "        axes[0, 1].bar(x_pos + width/2, pruned_memory, width, label='Pruned', alpha=0.8)\n",
    "        \n",
    "        axes[0, 1].set_title('Memory Usage Comparison', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Models')\n",
    "        axes[0, 1].set_ylabel('Memory Usage (MB)')\n",
    "        axes[0, 1].set_xticks(x_pos)\n",
    "        axes[0, 1].set_xticklabels(models)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Speedup Analysis\n",
    "    pruned_only = df[df['variant'] != 'Original']\n",
    "    if not pruned_only.empty:\n",
    "        model_names = pruned_only['model']\n",
    "        speedups = pruned_only['speedup']\n",
    "        \n",
    "        bars = axes[1, 0].bar(range(len(model_names)), speedups, alpha=0.8)\n",
    "        axes[1, 0].axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='No speedup')\n",
    "        axes[1, 0].set_title('Inference Speedup from Pruning', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Models')\n",
    "        axes[1, 0].set_ylabel('Speedup Factor')\n",
    "        axes[1, 0].set_xticks(range(len(model_names)))\n",
    "        axes[1, 0].set_xticklabels(model_names)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, speedup) in enumerate(zip(bars, speedups)):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                           f'{speedup:.2f}x', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 4: Efficiency Score (Accuracy per MB)\n",
    "    efficiency_data = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row['total_memory_mb'] > 0:\n",
    "            efficiency = row['accuracy'] / row['total_memory_mb']\n",
    "            efficiency_data.append(efficiency)\n",
    "            labels.append(f\"{row['model']}\\n{row['variant']}\")\n",
    "    \n",
    "    if efficiency_data:\n",
    "        bars = axes[1, 1].bar(range(len(efficiency_data)), efficiency_data, alpha=0.8)\n",
    "        axes[1, 1].set_title('Model Efficiency\\n(Accuracy per MB)', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Model Variants')\n",
    "        axes[1, 1].set_ylabel('Efficiency (Acc/MB)')\n",
    "        axes[1, 1].set_xticks(range(len(labels)))\n",
    "        axes[1, 1].set_xticklabels(labels, rotation=45, ha='right')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, eff) in enumerate(zip(bars, efficiency_data)):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                           f'{eff:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive performance analysis\n",
    "print(\"INFERENCE SPEED AND MEMORY ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if models and pruned_models_50:\n",
    "    # Run performance analysis\n",
    "    performance_results = comprehensive_performance_analysis(models, pruned_models_50)\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_performance_analysis(performance_results)\n",
    "    \n",
    "    # Create summary table\n",
    "    perf_df = pd.DataFrame(performance_results)\n",
    "    \n",
    "    print(\"\\nPerformance Analysis Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    summary_cols = ['model', 'variant', 'accuracy', 'inference_time_ms', \n",
    "                   'total_memory_mb', 'speedup', 'memory_reduction']\n",
    "    \n",
    "    display_df = perf_df[summary_cols].copy()\n",
    "    display_df['accuracy'] = display_df['accuracy'].apply(lambda x: f\"{x:.2f}%\")\n",
    "    display_df['inference_time_ms'] = display_df['inference_time_ms'].apply(lambda x: f\"{x:.2f}ms\")\n",
    "    display_df['total_memory_mb'] = display_df['total_memory_mb'].apply(lambda x: f\"{x:.1f}MB\")\n",
    "    display_df['speedup'] = display_df['speedup'].apply(lambda x: f\"{x:.2f}x\")\n",
    "    display_df['memory_reduction'] = display_df['memory_reduction'].apply(lambda x: f\"{x:.1f}%\")\n",
    "    \n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Performance improvements summary\n",
    "    print(\"\\nPerformance Improvements from Pruning:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    for model in perf_df['model'].unique():\n",
    "        model_data = perf_df[perf_df['model'] == model]\n",
    "        pruned_data = model_data[model_data['variant'] != 'Original']\n",
    "        \n",
    "        if not pruned_data.empty:\n",
    "            pruned_row = pruned_data.iloc[0]\n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  Inference speedup: {pruned_row['speedup']:.2f}x\")\n",
    "            print(f\"  Memory reduction: {pruned_row['memory_reduction']:.1f}%\")\n",
    "            print(f\"  Parameter reduction: {(1 - pruned_row['parameters'] / perf_df[perf_df['model'] == model]['parameters'].iloc[0]) * 100:.1f}%\")\n",
    "            \n",
    "            if pruned_row['speedup'] > 1.1:\n",
    "                print(f\"  ✅ Significant speedup achieved\")\n",
    "            elif pruned_row['speedup'] > 1.0:\n",
    "                print(f\"  ⚠️  Modest speedup achieved\")\n",
    "            else:\n",
    "                print(f\"  ❌ No speedup (may be due to overhead)\")\n",
    "\nelse:\n",
    "    print(\"No models available for performance analysis.\")\n",
    "\n",
    "print(\"\\nPerformance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Conclusions\n",
    "\n",
    "Summarize pruning analysis results and provide practical recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL PRUNING ANALYSIS - SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Comprehensive summary based on all analyses\n",
    "print(\"\\n🔍 KEY FINDINGS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 1. Pruning Effectiveness\n",
    "print(\"\\n1. Pruning Effectiveness:\")\n",
    "if 'pruning_df' in locals() and not pruning_df.empty:\n",
    "    # Find best performing pruning configurations\n",
    "    best_results = []\n",
    "    \n",
    "    for model in pruning_df['model_name'].unique():\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        \n",
    "        # Find configuration with best accuracy retention at high compression\n",
    "        high_compression = model_data[model_data['pruning_ratio'] >= 0.5]\n",
    "        if not high_compression.empty:\n",
    "            best_config = high_compression.loc[high_compression['pruned_accuracy'].idxmax()]\n",
    "            best_results.append(best_config)\n",
    "    \n",
    "    if best_results:\n",
    "        avg_compression = np.mean([r['compression_ratio'] for r in best_results])\n",
    "        avg_acc_drop = np.mean([r['accuracy_drop'] for r in best_results])\n",
    "        \n",
    "        print(f\"   • Average achievable compression: {avg_compression:.1f}x\")\n",
    "        print(f\"   • Average accuracy drop: {avg_acc_drop:.2f}%\")\n",
    "        \n",
    "        if avg_acc_drop < 5:\n",
    "            print(f\"   ✅ Excellent pruning results with minimal accuracy loss\")\n",
    "        elif avg_acc_drop < 10:\n",
    "            print(f\"   ✅ Good pruning results with acceptable accuracy loss\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Moderate pruning results with noticeable accuracy loss\")\n",
    "        \n",
    "        # Model comparison\n",
    "        print(f\"\\n   Model-specific results:\")\n",
    "        for result in best_results:\n",
    "            print(f\"     {result['model_name']}: {result['compression_ratio']:.1f}x compression, \"\n",
    "                  f\"{result['accuracy_drop']:.2f}% accuracy drop\")\n",
    "\n",
    "# 2. Layer-wise Impact\n",
    "print(\"\\n2. Layer-wise Pruning Impact:\")\n",
    "if 'layer_stats_dict' in locals():\n",
    "    for model_name, layer_stats in layer_stats_dict.items():\n",
    "        conv_sparsities = [stat['sparsity'] for stat in layer_stats if stat['type'] == 'Conv2d']\n",
    "        linear_sparsities = [stat['sparsity'] for stat in layer_stats if stat['type'] == 'Linear']\n",
    "        \n",
    "        if conv_sparsities and linear_sparsities:\n",
    "            avg_conv_sparsity = np.mean(conv_sparsities) * 100\n",
    "            avg_linear_sparsity = np.mean(linear_sparsities) * 100\n",
    "            \n",
    "            print(f\"   • {model_name}:\")\n",
    "            print(f\"     Conv layers: {avg_conv_sparsity:.1f}% average sparsity\")\n",
    "            print(f\"     Linear layers: {avg_linear_sparsity:.1f}% average sparsity\")\n",
    "            \n",
    "            if abs(avg_conv_sparsity - avg_linear_sparsity) < 10:\n",
    "                print(f\"     ✓ Uniform pruning across layer types\")\n",
    "            else:\n",
    "                print(f\"     ⚠️  Non-uniform pruning (may indicate layer sensitivity)\")\n",
    "\n",
    "# 3. Robustness Impact\n",
    "print(\"\\n3. Adversarial Robustness Impact:\")\n",
    "if 'robustness_comparison' in locals():\n",
    "    robustness_changes = [r['change'] for r in robustness_comparison]\n",
    "    \n",
    "    if robustness_changes:\n",
    "        avg_change = np.mean(robustness_changes)\n",
    "        print(f\"   • Average robustness change: {avg_change:+.2f}%\")\n",
    "        \n",
    "        if avg_change > 1:\n",
    "            print(f\"   ✅ Pruning improved adversarial robustness\")\n",
    "        elif avg_change > -2:\n",
    "            print(f\"   ✓ Minimal impact on adversarial robustness\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Pruning reduced adversarial robustness\")\n",
    "        \n",
    "        # Per-attack analysis\n",
    "        attack_summary = {}\n",
    "        for r in robustness_comparison:\n",
    "            if r['attack'] not in attack_summary:\n",
    "                attack_summary[r['attack']] = []\n",
    "            attack_summary[r['attack']].append(r['change'])\n",
    "        \n",
    "        print(f\"   Attack-specific changes:\")\n",
    "        for attack, changes in attack_summary.items():\n",
    "            avg_attack_change = np.mean(changes)\n",
    "            print(f\"     {attack}: {avg_attack_change:+.2f}%\")\nelse:\n",
    "    print(f\"   • No robustness analysis data available\")\n",
    "\n",
    "# 4. Performance Improvements\n",
    "print(\"\\n4. Performance Improvements:\")\n",
    "if 'performance_results' in locals():\n",
    "    pruned_results = [r for r in performance_results if r['variant'] != 'Original']\n",
    "    \n",
    "    if pruned_results:\n",
    "        avg_speedup = np.mean([r['speedup'] for r in pruned_results])\n",
    "        avg_memory_reduction = np.mean([r['memory_reduction'] for r in pruned_results])\n",
    "        \n",
    "        print(f\"   • Average inference speedup: {avg_speedup:.2f}x\")\n",
    "        print(f\"   • Average memory reduction: {avg_memory_reduction:.1f}%\")\n",
    "        \n",
    "        if avg_speedup > 1.5:\n",
    "            print(f\"   ✅ Significant performance improvements\")\n",
    "        elif avg_speedup > 1.1:\n",
    "            print(f\"   ✓ Moderate performance improvements\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Limited performance improvements (overhead may dominate)\")\nelse:\n",
    "    print(f\"   • No performance analysis data available\")\n",
    "\n",
    "# 5. Advanced Techniques\n",
    "print(\"\\n5. Advanced Pruning Techniques:\")\n",
    "if 'gradual_results' in locals():\n",
    "    final_gradual = gradual_results[-1]\n",
    "    initial_acc = gradual_results[0]['accuracy']\n",
    "    \n",
    "    gradual_acc_drop = initial_acc - final_gradual['accuracy']\n",
    "    \n",
    "    print(f\"   • Gradual pruning results:\")\n",
    "    print(f\"     Final compression: {final_gradual['compression_ratio']:.1f}x\")\n",
    "    print(f\"     Accuracy drop: {gradual_acc_drop:.2f}%\")\n",
    "    \n",
    "    # Compare with one-shot if available\n",
    "    if 'oneshot_acc' in locals():\n",
    "        oneshot_drop = initial_acc - oneshot_acc\n",
    "        improvement = oneshot_drop - gradual_acc_drop\n",
    "        \n",
    "        print(f\"     vs One-shot pruning: {improvement:+.2f}% better accuracy retention\")\n",
    "        \n",
    "        if improvement > 1:\n",
    "            print(f\"   ✅ Gradual pruning shows clear advantages\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Gradual vs one-shot pruning shows similar results\")\n",
    "\n",
    "# Practical Recommendations\n",
    "print(\"\\n📋 PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"\\n1. Optimal Pruning Strategy:\")\n",
    "if 'pruning_df' in locals() and not pruning_df.empty:\n",
    "    # Find sweet spot for each model\n",
    "    for model in pruning_df['model_name'].unique():\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        \n",
    "        # Find best balance (compression vs accuracy)\n",
    "        model_data = model_data.copy()\n",
    "        model_data['efficiency_score'] = model_data['compression_ratio'] / (1 + model_data['accuracy_drop'])\n",
    "        best_config = model_data.loc[model_data['efficiency_score'].idxmax()]\n",
    "        \n",
    "        print(f\"   • {model}: {best_config['pruning_ratio']:.1f} pruning ratio\")\n",
    "        print(f\"     Achieves {best_config['compression_ratio']:.1f}x compression with {best_config['accuracy_drop']:.2f}% accuracy drop\")\n",
    "\n",
    "print(\"\\n2. Implementation Guidelines:\")\n",
    "print(\"   • Start with magnitude-based unstructured pruning\")\n",
    "print(\"   • Use gradual pruning for better accuracy retention\")\n",
    "print(\"   • Target 50-70% sparsity for good compression without major accuracy loss\")\n",
    "print(\"   • Monitor both clean accuracy and adversarial robustness\")\n",
    "print(\"   • Consider layer sensitivity when applying structured pruning\")\n",
    "\n",
    "print(\"\\n3. Hardware Considerations:\")\n",
    "print(\"   • Unstructured pruning may not always improve inference speed\")\n",
    "print(\"   • Structured pruning typically provides better speedups\")\n",
    "print(\"   • Memory reduction is often more consistent than speed improvements\")\n",
    "print(\"   • Consider target deployment hardware when choosing pruning strategy\")\n",
    "\n",
    "print(\"\\n4. Model-Specific Insights:\")\n",
    "if 'pruning_df' in locals() and not pruning_df.empty:\n",
    "    for model in pruning_df['model_name'].unique():\n",
    "        model_data = pruning_df[pruning_df['model_name'] == model]\n",
    "        max_compression = model_data['compression_ratio'].max()\n",
    "        min_acc_drop = model_data[model_data['compression_ratio'] > 2]['accuracy_drop'].min()\n",
    "        \n",
    "        print(f\"   • {model}:\")\n",
    "        print(f\"     Max compression: {max_compression:.1f}x\")\n",
    "        print(f\"     Min accuracy drop at >2x compression: {min_acc_drop:.2f}%\")\n",
    "        \n",
    "        if min_acc_drop < 5:\n",
    "            print(f\"     ✓ Highly compressible model\")\n",
    "        elif min_acc_drop < 10:\n",
    "            print(f\"     ✓ Moderately compressible model\")\n",
    "        else:\n",
    "            print(f\"     ⚠️  Less compressible - careful tuning needed\")\n",
    "\n",
    "# Future Work and Advanced Techniques\n",
    "print(\"\\n🔬 FUTURE WORK AND ADVANCED TECHNIQUES:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(\"\\n1. Advanced Pruning Methods:\")\n",
    "print(\"   • Lottery ticket hypothesis - find winning subnetworks\")\n",
    "print(\"   • SNIP (Single-shot Network Pruning at Initialization)\")\n",
    "print(\"   • Magnitude-based structured pruning with automatic search\")\n",
    "print(\"   • Neural architecture search for optimal sparse architectures\")\n",
    "\n",
    "print(\"\\n2. Hardware-Aware Pruning:\")\n",
    "print(\"   • Target specific hardware accelerators (TPUs, mobile chips)\")\n",
    "print(\"   • Consider memory bandwidth and cache optimization\")\n",
    "print(\"   • Balance between model size and computational complexity\")\n",
    "\n",
    "print(\"\\n3. Joint Optimization:\")\n",
    "print(\"   • Combine pruning with quantization for maximum compression\")\n",
    "print(\"   • Knowledge distillation from unpruned to pruned models\")\n",
    "print(\"   • Adversarial training on pruned models for robust compression\")\n",
    "\n",
    "print(\"\\n4. Domain-Specific Considerations:\")\n",
    "print(\"   • Sports classification may benefit from preserving motion-related features\")\n",
    "print(\"   • Consider pruning impact on important visual features\")\n",
    "print(\"   • Evaluate pruning on different sports categories\")\n",
    "\n",
    "# Implementation Priority\n",
    "print(\"\\n📊 IMPLEMENTATION PRIORITY:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if 'pruning_df' in locals() and not pruning_df.empty:\n",
    "    avg_accuracy_drop = pruning_df[pruning_df['pruning_ratio'] == 0.5]['accuracy_drop'].mean()\n",
    "    \n",
    "    print(f\"\\nCurrent pruning performance at 50% sparsity: {avg_accuracy_drop:.2f}% average accuracy drop\")\n",
    "    \n",
    "    if avg_accuracy_drop < 3:\n",
    "        print(\"\\n🚀 HIGH PRIORITY - Excellent pruning candidates:\")\n",
    "        print(\"   1. Deploy pruned models immediately\")\n",
    "        print(\"   2. Explore higher compression ratios\")\n",
    "        print(\"   3. Implement structured pruning for hardware optimization\")\n",
    "    elif avg_accuracy_drop < 7:\n",
    "        print(\"\\n✅ MEDIUM PRIORITY - Good pruning potential:\")\n",
    "        print(\"   1. Fine-tune pruning ratios per layer\")\n",
    "        print(\"   2. Implement gradual pruning\")\n",
    "        print(\"   3. Consider accuracy-robustness trade-offs\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  LOW PRIORITY - Challenging pruning case:\")\n",
    "        print(\"   1. Focus on improving base model first\")\n",
    "        print(\"   2. Explore alternative compression methods\")\n",
    "        print(\"   3. Consider knowledge distillation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎯 Model pruning analysis complete!\")\n",
    "print(\"📊 Use these insights to optimize model deployment efficiency.\")\n",
    "print(\"⚡ Remember: The best pruning strategy depends on your specific deployment constraints.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
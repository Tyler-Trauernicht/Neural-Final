{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation\n",
    "\n",
    "## EE4745 Neural Networks Final Project\n",
    "\n",
    "This notebook demonstrates the complete training pipeline for sports image classification models.\n",
    "\n",
    "### Objectives:\n",
    "- Model architecture comparison and selection\n",
    "- Training process demonstration with real-time monitoring\n",
    "- Hyperparameter experimentation\n",
    "- Model evaluation and performance analysis\n",
    "- Training visualization and convergence analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import necessary libraries and set up the training environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from dataset.sports_dataset import SportsDataset, get_dataloaders\n",
    "from models.simple_cnn import SimpleCNN, create_simple_cnn\n",
    "from models.resnet_small import ResNetSmall, create_resnet_small\n",
    "from training.trainer import Trainer\n",
    "from training.utils import set_seed, get_device, count_parameters, format_time\n",
    "\n",
    "# Set style and configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Training Environment Setup\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Load and prepare the dataset for training with optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = '../data'\n",
    "IMAGE_SIZE = 32  # Start with 32x32 for faster training\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_loader, val_loader, num_classes = get_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Training samples: {len(train_loader.dataset):,}\")\n",
    "print(f\"  Validation samples: {len(val_loader.dataset):,}\")\n",
    "print(f\"  Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Class names\n",
    "class_names = SportsDataset.CLASSES\n",
    "print(f\"\\nClasses: {class_names}\")\n",
    "\n",
    "# Test data loading\n",
    "print(\"\\nTesting data loading...\")\n",
    "start_time = time.time()\n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    if batch_idx == 0:\n",
    "        print(f\"  Sample batch shape: {images.shape}\")\n",
    "        print(f\"  Label batch shape: {labels.shape}\")\n",
    "        print(f\"  Image range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "        print(f\"  Sample labels: {labels[:8].tolist()}\")\n",
    "        break\n",
    "load_time = time.time() - start_time\n",
    "print(f\"  First batch load time: {load_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Comparison\n",
    "\n",
    "Compare different model architectures and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_architecture(model, model_name, input_size=32):\n",
    "    \"\"\"Analyze and display model architecture information\"\"\"\n",
    "    \n",
    "    print(f\"\\n{model_name} Architecture Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Parameter count\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Model size estimation\n",
    "    param_size = total_params * 4 / (1024 * 1024)  # Assuming float32\n",
    "    print(f\"Estimated model size: {param_size:.2f} MB\")\n",
    "    \n",
    "    # Test forward pass\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 3, input_size, input_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        output = model(dummy_input)\n",
    "        inference_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Inference time: {inference_time*1000:.2f}ms\")\n",
    "    \n",
    "    # Architecture summary\n",
    "    print(f\"\\nArchitecture Summary:\")\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Only leaf modules\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "            if params > 0:\n",
    "                print(f\"  {name:30}: {str(module):50} | {params:,} params\")\n",
    "\n",
    "# Create and analyze models\n",
    "print(\"Model Architecture Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# SimpleCNN\n",
    "simple_cnn = create_simple_cnn(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "analyze_model_architecture(simple_cnn, \"SimpleCNN\", IMAGE_SIZE)\n",
    "\n",
    "# ResNetSmall\n",
    "resnet_small = create_resnet_small(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "analyze_model_architecture(resnet_small, \"ResNetSmall\", IMAGE_SIZE)\n",
    "\n",
    "# Comparison table\n",
    "models_comparison = {\n",
    "    'Model': ['SimpleCNN', 'ResNetSmall'],\n",
    "    'Parameters': [count_parameters(simple_cnn)[0], count_parameters(resnet_small)[0]],\n",
    "    'Size (MB)': [count_parameters(simple_cnn)[0] * 4 / (1024 * 1024), \n",
    "                  count_parameters(resnet_small)[0] * 4 / (1024 * 1024)]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_comparison)\n",
    "comparison_df['Parameters'] = comparison_df['Parameters'].apply(lambda x: f\"{x:,}\")\n",
    "comparison_df['Size (MB)'] = comparison_df['Size (MB)'].apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Configuration\n",
    "\n",
    "Define training configurations for different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base training configuration\n",
    "def get_base_config(model_name, experiment_name=None):\n",
    "    \"\"\"Get base training configuration\"\"\"\n",
    "    \n",
    "    if experiment_name is None:\n",
    "        experiment_name = f\"{model_name.lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    config = {\n",
    "        'model_name': model_name,\n",
    "        'experiment_name': experiment_name,\n",
    "        'device': str(device),\n",
    "        'epochs': 50,\n",
    "        'learning_rate': 1e-3,\n",
    "        'weight_decay': 1e-4,\n",
    "        'optimizer': 'adam',\n",
    "        'scheduler': 'cosine',\n",
    "        'eta_min': 1e-6,\n",
    "        'patience': 10,\n",
    "        'min_delta': 0.001,\n",
    "        'use_tensorboard': True,\n",
    "        'log_dir': '../logs',\n",
    "        'checkpoint_dir': '../checkpoints',\n",
    "        'save_best_only': True\n",
    "    }\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Training configurations for different experiments\n",
    "training_configs = {\n",
    "    'simple_cnn_baseline': {\n",
    "        **get_base_config('SimpleCNN', 'simple_cnn_baseline'),\n",
    "        'epochs': 30,\n",
    "        'learning_rate': 1e-3\n",
    "    },\n",
    "    'resnet_baseline': {\n",
    "        **get_base_config('ResNetSmall', 'resnet_baseline'),\n",
    "        'epochs': 40,\n",
    "        'learning_rate': 1e-3\n",
    "    },\n",
    "    'simple_cnn_tuned': {\n",
    "        **get_base_config('SimpleCNN', 'simple_cnn_tuned'),\n",
    "        'epochs': 40,\n",
    "        'learning_rate': 5e-4,\n",
    "        'weight_decay': 5e-4,\n",
    "        'scheduler': 'step',\n",
    "        'step_size': 15,\n",
    "        'gamma': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Training Configurations:\")\n",
    "print(\"=\" * 30)\n",
    "for config_name, config in training_configs.items():\n",
    "    print(f\"\\n{config_name}:\")\n",
    "    for key, value in config.items():\n",
    "        if key not in ['log_dir', 'checkpoint_dir', 'use_tensorboard', 'save_best_only']:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('../logs', exist_ok=True)\n",
    "os.makedirs('../checkpoints', exist_ok=True)\n",
    "print(\"\\nCreated necessary directories for logging and checkpoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Experiment 1: SimpleCNN Baseline\n",
    "\n",
    "Train a SimpleCNN model as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, verbose=True):\n",
    "    \"\"\"Train a model with the given configuration\"\"\"\n",
    "    \n",
    "    print(f\"\\nStarting training: {config['experiment_name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(model, train_loader, val_loader, config)\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = trainer.train()\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nTraining completed in {format_time(total_time)}\")\n",
    "    \n",
    "    return trainer, history\n",
    "\n",
    "# Experiment 1: SimpleCNN Baseline\n",
    "print(\"EXPERIMENT 1: SimpleCNN Baseline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create model\n",
    "model_simple = create_simple_cnn(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "config_simple = training_configs['simple_cnn_baseline']\n",
    "\n",
    "# Display training configuration\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in config_simple.items():\n",
    "    if key not in ['log_dir', 'checkpoint_dir', 'use_tensorboard', 'save_best_only']:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train model\n",
    "trainer_simple, history_simple = train_model(model_simple, train_loader, val_loader, config_simple)\n",
    "\n",
    "# Save results\n",
    "results_simple = {\n",
    "    'config': config_simple,\n",
    "    'history': history_simple,\n",
    "    'final_train_acc': history_simple['train_acc'][-1],\n",
    "    'final_val_acc': history_simple['val_acc'][-1],\n",
    "    'best_val_acc': max(history_simple['val_acc']),\n",
    "    'final_train_loss': history_simple['train_loss'][-1],\n",
    "    'final_val_loss': history_simple['val_loss'][-1],\n",
    "    'min_val_loss': min(history_simple['val_loss'])\n",
    "}\n",
    "\n",
    "print(f\"\\nSimpleCNN Results:\")\n",
    "print(f\"  Best validation accuracy: {results_simple['best_val_acc']:.2f}%\")\n",
    "print(f\"  Final validation accuracy: {results_simple['final_val_acc']:.2f}%\")\n",
    "print(f\"  Minimum validation loss: {results_simple['min_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Experiment 2: ResNetSmall\n",
    "\n",
    "Train a ResNetSmall model to compare with the SimpleCNN baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: ResNetSmall Baseline\n",
    "print(\"\\nEXPERIMENT 2: ResNetSmall Baseline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create model\n",
    "model_resnet = create_resnet_small(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "config_resnet = training_configs['resnet_baseline']\n",
    "\n",
    "# Display training configuration\n",
    "print(\"\\nTraining Configuration:\")\n",
    "for key, value in config_resnet.items():\n",
    "    if key not in ['log_dir', 'checkpoint_dir', 'use_tensorboard', 'save_best_only']:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train model\n",
    "trainer_resnet, history_resnet = train_model(model_resnet, train_loader, val_loader, config_resnet)\n",
    "\n",
    "# Save results\n",
    "results_resnet = {\n",
    "    'config': config_resnet,\n",
    "    'history': history_resnet,\n",
    "    'final_train_acc': history_resnet['train_acc'][-1],\n",
    "    'final_val_acc': history_resnet['val_acc'][-1],\n",
    "    'best_val_acc': max(history_resnet['val_acc']),\n",
    "    'final_train_loss': history_resnet['train_loss'][-1],\n",
    "    'final_val_loss': history_resnet['val_loss'][-1],\n",
    "    'min_val_loss': min(history_resnet['val_loss'])\n",
    "}\n",
    "\n",
    "print(f\"\\nResNetSmall Results:\")\n",
    "print(f\"  Best validation accuracy: {results_resnet['best_val_acc']:.2f}%\")\n",
    "print(f\"  Final validation accuracy: {results_resnet['final_val_acc']:.2f}%\")\n",
    "print(f\"  Minimum validation loss: {results_resnet['min_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization and Analysis\n",
    "\n",
    "Visualize training curves and analyze model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(experiments_dict, title=\"Training Curves Comparison\"):\n",
    "    \"\"\"Plot training curves for multiple experiments\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    for i, (name, results) in enumerate(experiments_dict.items()):\n",
    "        epochs = range(1, len(results['history']['train_loss']) + 1)\n",
    "        axes[0, 0].plot(epochs, results['history']['train_loss'], \n",
    "                       color=colors[i], label=f'{name}', linewidth=2)\n",
    "    axes[0, 0].set_title('Training Loss', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Validation Loss\n",
    "    for i, (name, results) in enumerate(experiments_dict.items()):\n",
    "        epochs = range(1, len(results['history']['val_loss']) + 1)\n",
    "        axes[0, 1].plot(epochs, results['history']['val_loss'], \n",
    "                       color=colors[i], label=f'{name}', linewidth=2)\n",
    "    axes[0, 1].set_title('Validation Loss', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Training Accuracy\n",
    "    for i, (name, results) in enumerate(experiments_dict.items()):\n",
    "        epochs = range(1, len(results['history']['train_acc']) + 1)\n",
    "        axes[1, 0].plot(epochs, results['history']['train_acc'], \n",
    "                       color=colors[i], label=f'{name}', linewidth=2)\n",
    "    axes[1, 0].set_title('Training Accuracy', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Validation Accuracy\n",
    "    for i, (name, results) in enumerate(experiments_dict.items()):\n",
    "        epochs = range(1, len(results['history']['val_acc']) + 1)\n",
    "        axes[1, 1].plot(epochs, results['history']['val_acc'], \n",
    "                       color=colors[i], label=f'{name}', linewidth=2)\n",
    "    axes[1, 1].set_title('Validation Accuracy', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Collect all experiment results\n",
    "all_experiments = {\n",
    "    'SimpleCNN': results_simple,\n",
    "    'ResNetSmall': results_resnet\n",
    "}\n",
    "\n",
    "# Plot training curves\n",
    "plot_training_curves(all_experiments, \"Model Training Comparison\")\n",
    "\n",
    "# Learning rate analysis\n",
    "def plot_learning_curves_detailed(history, title):\n",
    "    \"\"\"Plot detailed learning curves for a single model\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{title} - Detailed Training Analysis', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 0].set_title('Loss Curves')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Training', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Validation', linewidth=2)\n",
    "    axes[0, 1].set_title('Accuracy Curves')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss smoothing (moving average)\n",
    "    window = 3\n",
    "    if len(history['train_loss']) >= window:\n",
    "        train_loss_smooth = pd.Series(history['train_loss']).rolling(window).mean()\n",
    "        val_loss_smooth = pd.Series(history['val_loss']).rolling(window).mean()\n",
    "        \n",
    "        axes[1, 0].plot(epochs, train_loss_smooth, 'b-', label='Training (smoothed)', linewidth=2)\n",
    "        axes[1, 0].plot(epochs, val_loss_smooth, 'r-', label='Validation (smoothed)', linewidth=2)\n",
    "    else:\n",
    "        axes[1, 0].plot(epochs, history['train_loss'], 'b-', label='Training', linewidth=2)\n",
    "        axes[1, 0].plot(epochs, history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "    \n",
    "    axes[1, 0].set_title('Smoothed Loss Curves')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Training-Validation gap analysis\n",
    "    acc_gap = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "    loss_gap = [v - t for t, v in zip(history['train_loss'], history['val_loss'])]\n",
    "    \n",
    "    ax2 = axes[1, 1]\n",
    "    ax2.plot(epochs, acc_gap, 'g-', linewidth=2, label='Accuracy Gap')\n",
    "    ax2.set_ylabel('Accuracy Gap (Train - Val)', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(epochs, loss_gap, 'orange', linewidth=2, label='Loss Gap')\n",
    "    ax2_twin.set_ylabel('Loss Gap (Val - Train)', color='orange')\n",
    "    ax2_twin.tick_params(axis='y', labelcolor='orange')\n",
    "    \n",
    "    ax2.set_title('Overfitting Analysis')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot detailed curves for each model\n",
    "plot_learning_curves_detailed(history_simple, \"SimpleCNN\")\n",
    "plot_learning_curves_detailed(history_resnet, \"ResNetSmall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Testing\n",
    "\n",
    "Evaluate trained models and generate detailed performance reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, class_names):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        all_labels, all_predictions, average=None\n",
    "    )\n",
    "    \n",
    "    # Classification report\n",
    "    class_report = classification_report(\n",
    "        all_labels, all_predictions, \n",
    "        target_names=class_names, \n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    return {\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels,\n",
    "        'probabilities': np.array(all_probs),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names, title, normalize=False):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.nan_to_num(cm)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd', \n",
    "                cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_classification_report(class_report, class_names, title):\n",
    "    \"\"\"Visualize classification report as bar plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [class_report[cls][metric] for cls in class_names]\n",
    "        \n",
    "        bars = axes[i].bar(range(len(class_names)), values, alpha=0.8)\n",
    "        axes[i].set_title(f'{metric.capitalize()}', fontweight='bold')\n",
    "        axes[i].set_xlabel('Classes')\n",
    "        axes[i].set_ylabel(metric.capitalize())\n",
    "        axes[i].set_xticks(range(len(class_names)))\n",
    "        axes[i].set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        axes[i].set_ylim(0, 1.0)\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{values[j]:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate SimpleCNN\n",
    "print(\"\\nEVALUATING SIMPLECNN MODEL\")\n",
    "print(\"=\" * 40)\n",
    "simple_eval = evaluate_model(trainer_simple.get_model(), val_loader, device, class_names)\n",
    "\n",
    "print(f\"SimpleCNN Validation Results:\")\n",
    "print(f\"  Accuracy: {simple_eval['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {simple_eval['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"  Weighted F1: {simple_eval['classification_report']['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# Evaluate ResNetSmall\n",
    "print(\"\\nEVALUATING RESNETSMALL MODEL\")\n",
    "print(\"=\" * 40)\n",
    "resnet_eval = evaluate_model(trainer_resnet.get_model(), val_loader, device, class_names)\n",
    "\n",
    "print(f\"ResNetSmall Validation Results:\")\n",
    "print(f\"  Accuracy: {resnet_eval['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {resnet_eval['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"  Weighted F1: {resnet_eval['classification_report']['weighted avg']['f1-score']:.4f}\")\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(simple_eval['confusion_matrix'], class_names, \n",
    "                     \"SimpleCNN - Confusion Matrix\")\n",
    "plot_confusion_matrix(simple_eval['confusion_matrix'], class_names, \n",
    "                     \"SimpleCNN - Normalized Confusion Matrix\", normalize=True)\n",
    "\n",
    "plot_confusion_matrix(resnet_eval['confusion_matrix'], class_names, \n",
    "                     \"ResNetSmall - Confusion Matrix\")\n",
    "plot_confusion_matrix(resnet_eval['confusion_matrix'], class_names, \n",
    "                     \"ResNetSmall - Normalized Confusion Matrix\", normalize=True)\n",
    "\n",
    "# Plot classification reports\n",
    "plot_classification_report(simple_eval['classification_report'], class_names, \n",
    "                          \"SimpleCNN - Per-Class Performance\")\n",
    "plot_classification_report(resnet_eval['classification_report'], class_names, \n",
    "                          \"ResNetSmall - Per-Class Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning Experiment\n",
    "\n",
    "Demonstrate hyperparameter tuning with the SimpleCNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: SimpleCNN with tuned hyperparameters\n",
    "print(\"EXPERIMENT 3: SimpleCNN Hyperparameter Tuning\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create new model with same architecture\n",
    "model_tuned = create_simple_cnn(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "config_tuned = training_configs['simple_cnn_tuned']\n",
    "\n",
    "print(\"\\nTuned Configuration Changes:\")\n",
    "print(f\"  Learning rate: {config_tuned['learning_rate']} (vs {config_simple['learning_rate']})\")\n",
    "print(f\"  Weight decay: {config_tuned['weight_decay']} (vs {config_simple['weight_decay']})\")\n",
    "print(f\"  Scheduler: {config_tuned['scheduler']} (vs {config_simple['scheduler']})\")\n",
    "print(f\"  Epochs: {config_tuned['epochs']} (vs {config_simple['epochs']})\")\n",
    "\n",
    "# Train tuned model\n",
    "trainer_tuned, history_tuned = train_model(model_tuned, train_loader, val_loader, config_tuned)\n",
    "\n",
    "# Save results\n",
    "results_tuned = {\n",
    "    'config': config_tuned,\n",
    "    'history': history_tuned,\n",
    "    'final_train_acc': history_tuned['train_acc'][-1],\n",
    "    'final_val_acc': history_tuned['val_acc'][-1],\n",
    "    'best_val_acc': max(history_tuned['val_acc']),\n",
    "    'final_train_loss': history_tuned['train_loss'][-1],\n",
    "    'final_val_loss': history_tuned['val_loss'][-1],\n",
    "    'min_val_loss': min(history_tuned['val_loss'])\n",
    "}\n",
    "\n",
    "print(f\"\\nTuned SimpleCNN Results:\")\n",
    "print(f\"  Best validation accuracy: {results_tuned['best_val_acc']:.2f}%\")\n",
    "print(f\"  Final validation accuracy: {results_tuned['final_val_acc']:.2f}%\")\n",
    "print(f\"  Minimum validation loss: {results_tuned['min_val_loss']:.4f}\")\n",
    "\n",
    "# Add to experiments\n",
    "all_experiments['SimpleCNN_Tuned'] = results_tuned\n",
    "\n",
    "# Compare all three models\n",
    "plot_training_curves(all_experiments, \"Complete Model Comparison\")\n",
    "\n",
    "# Evaluate tuned model\n",
    "print(\"\\nEVALUATING TUNED SIMPLECNN MODEL\")\n",
    "print(\"=\" * 40)\n",
    "tuned_eval = evaluate_model(trainer_tuned.get_model(), val_loader, device, class_names)\n",
    "\n",
    "print(f\"Tuned SimpleCNN Validation Results:\")\n",
    "print(f\"  Accuracy: {tuned_eval['accuracy']:.4f}\")\n",
    "print(f\"  Macro F1: {tuned_eval['classification_report']['macro avg']['f1-score']:.4f}\")\n",
    "print(f\"  Weighted F1: {tuned_eval['classification_report']['weighted avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Analysis\n",
    "\n",
    "Comprehensive comparison of all trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "def create_model_comparison_table(experiments, evaluations):\n",
    "    \"\"\"Create a comprehensive comparison table\"\"\"\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    model_names = ['SimpleCNN', 'ResNetSmall', 'SimpleCNN_Tuned']\n",
    "    eval_names = [simple_eval, resnet_eval, tuned_eval]\n",
    "    \n",
    "    for i, (model_name, eval_result) in enumerate(zip(model_names, eval_names)):\n",
    "        if model_name in experiments:\n",
    "            exp_data = experiments[model_name]\n",
    "            \n",
    "            row = {\n",
    "                'Model': model_name,\n",
    "                'Best Val Acc (%)': f\"{exp_data['best_val_acc']:.2f}\",\n",
    "                'Final Val Acc (%)': f\"{exp_data['final_val_acc']:.2f}\",\n",
    "                'Min Val Loss': f\"{exp_data['min_val_loss']:.4f}\",\n",
    "                'Test Accuracy': f\"{eval_result['accuracy']:.4f}\",\n",
    "                'Macro F1': f\"{eval_result['classification_report']['macro avg']['f1-score']:.4f}\",\n",
    "                'Weighted F1': f\"{eval_result['classification_report']['weighted avg']['f1-score']:.4f}\",\n",
    "                'Epochs Trained': len(exp_data['history']['train_loss'])\n",
    "            }\n",
    "            comparison_data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = create_model_comparison_table(\n",
    "    all_experiments, \n",
    "    [simple_eval, resnet_eval, tuned_eval]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Performance improvement analysis\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "baseline_acc = results_simple['best_val_acc']\n",
    "resnet_acc = results_resnet['best_val_acc']\n",
    "tuned_acc = results_tuned['best_val_acc']\n",
    "\n",
    "print(f\"\\nBaseline SimpleCNN: {baseline_acc:.2f}%\")\n",
    "print(f\"ResNetSmall vs Baseline: {resnet_acc:.2f}% ({resnet_acc - baseline_acc:+.2f}%)\")\n",
    "print(f\"Tuned SimpleCNN vs Baseline: {tuned_acc:.2f}% ({tuned_acc - baseline_acc:+.2f}%)\")\n",
    "\n",
    "if resnet_acc > baseline_acc:\n",
    "    print(f\"\\n‚úÖ ResNetSmall shows improvement over baseline\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è ResNetSmall does not improve over baseline\")\n",
    "\n",
    "if tuned_acc > baseline_acc:\n",
    "    print(f\"‚úÖ Hyperparameter tuning shows improvement\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Hyperparameter tuning does not improve performance\")\n",
    "\n",
    "# Best performing model\n",
    "best_model = max(all_experiments.keys(), \n",
    "                key=lambda x: all_experiments[x]['best_val_acc'])\n",
    "best_acc = all_experiments[best_model]['best_val_acc']\n",
    "\n",
    "print(f\"\\nüèÜ Best performing model: {best_model} ({best_acc:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Learning Rate and Optimizer Analysis\n",
    "\n",
    "Analyze the effect of different learning rates and optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_training_experiment(model_fn, config_base, modifications, experiment_name):\n",
    "    \"\"\"Run a quick training experiment with modifications\"\"\"\n",
    "    \n",
    "    # Create modified config\n",
    "    config = config_base.copy()\n",
    "    config.update(modifications)\n",
    "    config['experiment_name'] = experiment_name\n",
    "    config['epochs'] = 15  # Shorter training for quick experiments\n",
    "    \n",
    "    # Create model\n",
    "    model = model_fn(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "    \n",
    "    print(f\"\\nQuick experiment: {experiment_name}\")\n",
    "    print(f\"Modifications: {modifications}\")\n",
    "    \n",
    "    # Train\n",
    "    trainer, history = train_model(model, train_loader, val_loader, config)\n",
    "    \n",
    "    return {\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'best_val_acc': max(history['val_acc']),\n",
    "        'final_val_acc': history['val_acc'][-1]\n",
    "    }\n",
    "\n",
    "print(\"LEARNING RATE AND OPTIMIZER EXPERIMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Base configuration for quick experiments\n",
    "quick_config = {\n",
    "    'model_name': 'SimpleCNN_Quick',\n",
    "    'device': str(device),\n",
    "    'epochs': 15,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'optimizer': 'adam',\n",
    "    'scheduler': 'none',\n",
    "    'patience': 10,\n",
    "    'min_delta': 0.001,\n",
    "    'use_tensorboard': False,\n",
    "    'log_dir': '../logs',\n",
    "    'checkpoint_dir': '../checkpoints'\n",
    "}\n",
    "\n",
    "# Experiment with different learning rates\n",
    "lr_experiments = {}\n",
    "learning_rates = [1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "\n",
    "print(\"\\nTesting different learning rates...\")\n",
    "for lr in learning_rates:\n",
    "    exp_name = f\"lr_{lr:.0e}\"\n",
    "    modifications = {'learning_rate': lr}\n",
    "    \n",
    "    try:\n",
    "        result = quick_training_experiment(\n",
    "            create_simple_cnn, quick_config, modifications, exp_name\n",
    "        )\n",
    "        lr_experiments[exp_name] = result\n",
    "        print(f\"  LR {lr:.0e}: Best Val Acc = {result['best_val_acc']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"  LR {lr:.0e}: Failed - {e}\")\n",
    "\n",
    "# Experiment with different optimizers\n",
    "optimizer_experiments = {}\n",
    "optimizers = ['adam', 'sgd']\n",
    "\n",
    "print(\"\\nTesting different optimizers...\")\n",
    "for opt in optimizers:\n",
    "    exp_name = f\"opt_{opt}\"\n",
    "    modifications = {'optimizer': opt, 'learning_rate': 1e-3}\n",
    "    if opt == 'sgd':\n",
    "        modifications['momentum'] = 0.9\n",
    "    \n",
    "    try:\n",
    "        result = quick_training_experiment(\n",
    "            create_simple_cnn, quick_config, modifications, exp_name\n",
    "        )\n",
    "        optimizer_experiments[exp_name] = result\n",
    "        print(f\"  {opt.upper()}: Best Val Acc = {result['best_val_acc']:.2f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"  {opt.upper()}: Failed - {e}\")\n",
    "\n",
    "# Visualize results\n",
    "if lr_experiments:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Learning rate comparison\n",
    "    lr_names = list(lr_experiments.keys())\n",
    "    lr_accs = [lr_experiments[name]['best_val_acc'] for name in lr_names]\n",
    "    lr_values = [float(name.split('_')[1]) for name in lr_names]\n",
    "    \n",
    "    axes[0].semilogx(lr_values, lr_accs, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0].set_title('Learning Rate vs Performance', fontweight='bold')\n",
    "    axes[0].set_xlabel('Learning Rate')\n",
    "    axes[0].set_ylabel('Best Validation Accuracy (%)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate points\n",
    "    for lr, acc in zip(lr_values, lr_accs):\n",
    "        axes[0].annotate(f'{acc:.1f}%', (lr, acc), \n",
    "                        textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    # Training curves for best LR\n",
    "    best_lr_exp = max(lr_experiments.values(), key=lambda x: x['best_val_acc'])\n",
    "    epochs = range(1, len(best_lr_exp['history']['val_acc']) + 1)\n",
    "    \n",
    "    axes[1].plot(epochs, best_lr_exp['history']['train_acc'], 'b-', label='Training')\n",
    "    axes[1].plot(epochs, best_lr_exp['history']['val_acc'], 'r-', label='Validation')\n",
    "    axes[1].set_title('Best Learning Rate - Training Curves', fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best learning rate\n",
    "    best_lr_name = max(lr_experiments.keys(), \n",
    "                       key=lambda x: lr_experiments[x]['best_val_acc'])\n",
    "    best_lr_value = float(best_lr_name.split('_')[1])\n",
    "    best_lr_acc = lr_experiments[best_lr_name]['best_val_acc']\n",
    "    \n",
    "    print(f\"\\nüéØ Optimal learning rate: {best_lr_value:.0e} (Val Acc: {best_lr_acc:.2f}%)\")\n",
    "\n",
    "# Summary of hyperparameter experiments\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if lr_experiments:\n",
    "    print(\"\\nLearning Rate Results:\")\n",
    "    for name, result in lr_experiments.items():\n",
    "        lr_val = float(name.split('_')[1])\n",
    "        print(f\"  {lr_val:.0e}: {result['best_val_acc']:.2f}%\")\n",
    "\n",
    "if optimizer_experiments:\n",
    "    print(\"\\nOptimizer Results:\")\n",
    "    for name, result in optimizer_experiments.items():\n",
    "        opt_name = name.split('_')[1].upper()\n",
    "        print(f\"  {opt_name}: {result['best_val_acc']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Training Summary and Conclusions\n",
    "\n",
    "Summarize all experiments and provide training recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING EXPERIMENT SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Main experiment results\n",
    "print(\"\\nüìä MAIN EXPERIMENT RESULTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for model_name, results in all_experiments.items():\n",
    "    config = results['config']\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best validation accuracy: {results['best_val_acc']:.2f}%\")\n",
    "    print(f\"  Final validation accuracy: {results['final_val_acc']:.2f}%\")\n",
    "    print(f\"  Minimum validation loss: {results['min_val_loss']:.4f}\")\n",
    "    print(f\"  Training epochs: {len(results['history']['train_loss'])}\")\n",
    "    print(f\"  Optimizer: {config['optimizer']}\")\n",
    "    print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "    print(f\"  Scheduler: {config['scheduler']}\")\n",
    "\n",
    "# Key findings\n",
    "print(\"\\nüîç KEY FINDINGS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Best model identification\n",
    "best_model = max(all_experiments.keys(), \n",
    "                key=lambda x: all_experiments[x]['best_val_acc'])\n",
    "best_acc = all_experiments[best_model]['best_val_acc']\n",
    "worst_model = min(all_experiments.keys(), \n",
    "                 key=lambda x: all_experiments[x]['best_val_acc'])\n",
    "worst_acc = all_experiments[worst_model]['best_val_acc']\n",
    "\n",
    "print(f\"\\n1. Model Performance Ranking:\")\n",
    "sorted_models = sorted(all_experiments.items(), \n",
    "                      key=lambda x: x[1]['best_val_acc'], \n",
    "                      reverse=True)\n",
    "for i, (name, results) in enumerate(sorted_models, 1):\n",
    "    print(f\"   {i}. {name}: {results['best_val_acc']:.2f}%\")\n",
    "\n",
    "print(f\"\\n2. Performance Range: {worst_acc:.2f}% - {best_acc:.2f}% ({best_acc - worst_acc:.2f}% spread)\")\n",
    "\n",
    "# Architecture comparison\n",
    "simple_models = [k for k in all_experiments.keys() if 'SimpleCNN' in k]\n",
    "resnet_models = [k for k in all_experiments.keys() if 'ResNet' in k]\n",
    "\n",
    "if simple_models and resnet_models:\n",
    "    best_simple = max(simple_models, key=lambda x: all_experiments[x]['best_val_acc'])\n",
    "    best_resnet = max(resnet_models, key=lambda x: all_experiments[x]['best_val_acc'])\n",
    "    \n",
    "    simple_acc = all_experiments[best_simple]['best_val_acc']\n",
    "    resnet_acc = all_experiments[best_resnet]['best_val_acc']\n",
    "    \n",
    "    print(f\"\\n3. Architecture Comparison:\")\n",
    "    print(f\"   Best SimpleCNN: {simple_acc:.2f}%\")\n",
    "    print(f\"   Best ResNet: {resnet_acc:.2f}%\")\n",
    "    if resnet_acc > simple_acc:\n",
    "        print(f\"   ‚úÖ ResNet shows {resnet_acc - simple_acc:.2f}% improvement\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è SimpleCNN performs {simple_acc - resnet_acc:.2f}% better\")\n",
    "\n",
    "# Hyperparameter tuning impact\n",
    "if 'SimpleCNN_Tuned' in all_experiments and 'SimpleCNN' in all_experiments:\n",
    "    baseline_acc = all_experiments['SimpleCNN']['best_val_acc']\n",
    "    tuned_acc = all_experiments['SimpleCNN_Tuned']['best_val_acc']\n",
    "    \n",
    "    print(f\"\\n4. Hyperparameter Tuning Impact:\")\n",
    "    print(f\"   Baseline: {baseline_acc:.2f}%\")\n",
    "    print(f\"   Tuned: {tuned_acc:.2f}%\")\n",
    "    if tuned_acc > baseline_acc:\n",
    "        print(f\"   ‚úÖ Tuning improved performance by {tuned_acc - baseline_acc:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Tuning decreased performance by {baseline_acc - tuned_acc:.2f}%\")\n",
    "\n",
    "# Learning rate findings\n",
    "if 'lr_experiments' in locals() and lr_experiments:\n",
    "    best_lr_exp = max(lr_experiments.values(), key=lambda x: x['best_val_acc'])\n",
    "    best_lr_name = max(lr_experiments.keys(), key=lambda x: lr_experiments[x]['best_val_acc'])\n",
    "    best_lr_value = float(best_lr_name.split('_')[1])\n",
    "    \n",
    "    print(f\"\\n5. Learning Rate Analysis:\")\n",
    "    print(f\"   Optimal LR: {best_lr_value:.0e}\")\n",
    "    print(f\"   Performance range: {min(lr_experiments.values(), key=lambda x: x['best_val_acc'])['best_val_acc']:.2f}% - {best_lr_exp['best_val_acc']:.2f}%\")\n",
    "\n",
    "# Training efficiency\n",
    "print(f\"\\n6. Training Efficiency:\")\n",
    "for name, results in all_experiments.items():\n",
    "    epochs_trained = len(results['history']['train_loss'])\n",
    "    config_epochs = results['config']['epochs']\n",
    "    completion_rate = epochs_trained / config_epochs * 100\n",
    "    print(f\"   {name}: {epochs_trained}/{config_epochs} epochs ({completion_rate:.0f}%)\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüéØ TRAINING RECOMMENDATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n1. Model Architecture:\")\n",
    "if resnet_acc > simple_acc:\n",
    "    print(\"   ‚úÖ Use ResNet architecture for better performance\")\n",
    "    print(\"   ‚Ä¢ ResNet's residual connections help with gradient flow\")\n",
    "    print(\"   ‚Ä¢ Better feature extraction capabilities\")\n",
    "else:\n",
    "    print(\"   ‚úÖ SimpleCNN is sufficient for this task\")\n",
    "    print(\"   ‚Ä¢ Faster training and inference\")\n",
    "    print(\"   ‚Ä¢ Lower computational requirements\")\n",
    "\n",
    "print(\"\\n2. Hyperparameter Settings:\")\n",
    "print(f\"   ‚Ä¢ Best performing config: {best_model}\")\n",
    "best_config = all_experiments[best_model]['config']\n",
    "print(f\"   ‚Ä¢ Learning rate: {best_config['learning_rate']}\")\n",
    "print(f\"   ‚Ä¢ Optimizer: {best_config['optimizer']}\")\n",
    "print(f\"   ‚Ä¢ Scheduler: {best_config['scheduler']}\")\n",
    "print(f\"   ‚Ä¢ Weight decay: {best_config['weight_decay']}\")\n",
    "\n",
    "if 'lr_experiments' in locals() and lr_experiments:\n",
    "    print(f\"   ‚Ä¢ Optimal learning rate from experiments: {best_lr_value:.0e}\")\n",
    "\n",
    "print(\"\\n3. Training Strategy:\")\n",
    "avg_epochs = np.mean([len(r['history']['train_loss']) for r in all_experiments.values()])\n",
    "print(f\"   ‚Ä¢ Training duration: ~{avg_epochs:.0f} epochs typically sufficient\")\n",
    "print(\"   ‚Ä¢ Use early stopping to prevent overfitting\")\n",
    "print(\"   ‚Ä¢ Monitor validation accuracy as primary metric\")\n",
    "print(\"   ‚Ä¢ Consider cosine annealing scheduler for better convergence\")\n",
    "\n",
    "print(\"\\n4. Further Improvements:\")\n",
    "print(\"   ‚Ä¢ Try different data augmentation strategies\")\n",
    "print(\"   ‚Ä¢ Experiment with larger image sizes (64x64 or 128x128)\")\n",
    "print(\"   ‚Ä¢ Consider ensemble methods\")\n",
    "print(\"   ‚Ä¢ Implement label smoothing for regularization\")\n",
    "print(\"   ‚Ä¢ Try different optimizers (AdamW, RMSprop)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"üèÜ BEST MODEL: {best_model} with {best_acc:.2f}% validation accuracy\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
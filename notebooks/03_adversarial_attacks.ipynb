{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Attacks Analysis\n",
    "\n",
    "## EE4745 Neural Networks Final Project\n",
    "\n",
    "This notebook provides comprehensive analysis of adversarial attacks against sports image classification models.\n",
    "\n",
    "### Objectives:\n",
    "- Implement and demonstrate FGSM and PGD adversarial attacks\n",
    "- Analyze attack effectiveness across different models\n",
    "- Visualize adversarial examples and perturbations\n",
    "- Study transferability of adversarial examples\n",
    "- Evaluate model robustness and interpretability\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Import libraries and set up the environment for adversarial attack analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "from dataset.sports_dataset import SportsDataset, get_dataloaders\n",
    "from models.simple_cnn import create_simple_cnn\n",
    "from models.resnet_small import create_resnet_small\n",
    "from training.utils import set_seed, get_device, load_checkpoint\n",
    "from attacks.fgsm import FGSM\n",
    "from attacks.pgd import PGD\n",
    "from attacks.transferability import TransferabilityAnalyzer\n",
    "from attacks.interpretability import AdversarialInterpretabilityAnalyzer\n",
    "from attacks import utils as attack_utils\n",
    "from interpretability.saliency import SaliencyMap\n",
    "from interpretability.gradcam import GradCAM, get_target_layer\n",
    "\n",
    "# Set style and configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette('Set1')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"Adversarial Attacks Analysis Setup\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Pre-trained Models\n",
    "\n",
    "Load the dataset and pre-trained models for attack analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset configuration\n",
    "DATA_DIR = '../data'\n",
    "IMAGE_SIZE = 32\n",
    "BATCH_SIZE = 16  # Smaller batch size for attack analysis\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets for attack analysis...\")\n",
    "train_loader, val_loader, num_classes = get_dataloaders(\n",
    "    data_dir=DATA_DIR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "class_names = SportsDataset.CLASSES\n",
    "print(f\"Dataset loaded: {num_classes} classes\")\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "\n",
    "# Create a small subset for detailed analysis\n",
    "subset_size = 100  # Small subset for detailed analysis\n",
    "subset_indices = torch.randperm(len(val_loader.dataset))[:subset_size]\n",
    "subset_dataset = torch.utils.data.Subset(val_loader.dataset, subset_indices)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Created subset with {subset_size} samples for detailed analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Models\n",
    "\n",
    "Load the trained models from previous experiments or create new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_model(model_type, checkpoint_path=None):\n",
    "    \"\"\"Load a pre-trained model or create a new one\"\"\"\n",
    "    \n",
    "    if model_type == 'SimpleCNN':\n",
    "        model = create_simple_cnn(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "    elif model_type == 'ResNetSmall':\n",
    "        model = create_resnet_small(num_classes=num_classes, input_size=IMAGE_SIZE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "        try:\n",
    "            load_checkpoint(checkpoint_path, model)\n",
    "            print(\"✅ Checkpoint loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Failed to load checkpoint: {e}\")\n",
    "            print(\"Using randomly initialized model\")\n",
    "    else:\n",
    "        print(f\"⚠️  No checkpoint found, using randomly initialized {model_type}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Define possible checkpoint paths\n",
    "checkpoint_paths = {\n",
    "    'SimpleCNN': '../checkpoints/SimpleCNN-best.pt',\n",
    "    'ResNetSmall': '../checkpoints/ResNetSmall-best.pt'\n",
    "}\n",
    "\n",
    "# Load models\n",
    "print(\"\\nLoading models...\")\n",
    "models = {}\n",
    "\n",
    "for model_type in ['SimpleCNN', 'ResNetSmall']:\n",
    "    print(f\"\\nLoading {model_type}...\")\n",
    "    models[model_type] = load_or_create_model(model_type, checkpoint_paths.get(model_type))\n",
    "    \n",
    "    # Quick evaluation on clean data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in subset_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = models[model_type](images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    clean_accuracy = 100.0 * correct / total\n",
    "    print(f\"  Clean accuracy on subset: {clean_accuracy:.2f}%\")\n",
    "\n",
    "print(f\"\\nModels loaded: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FGSM Attack Implementation and Analysis\n",
    "\n",
    "Demonstrate the Fast Gradient Sign Method (FGSM) attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_attack(model, attack_method, dataloader, attack_name, epsilon_values):\n",
    "    \"\"\"Evaluate attack effectiveness across different epsilon values\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for epsilon in epsilon_values:\n",
    "        print(f\"\\nEvaluating {attack_name} with epsilon={epsilon:.4f}...\")\n",
    "        \n",
    "        correct_clean = 0\n",
    "        correct_adv = 0\n",
    "        total = 0\n",
    "        \n",
    "        perturbation_norms = []\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader, desc=f\"ε={epsilon:.4f}\")):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Clean prediction\n",
    "            with torch.no_grad():\n",
    "                clean_outputs = model(images)\n",
    "                clean_pred = clean_outputs.argmax(dim=1)\n",
    "                correct_clean += (clean_pred == labels).sum().item()\n",
    "            \n",
    "            # Generate adversarial examples\n",
    "            if epsilon > 0:\n",
    "                adv_images = attack_method.attack(images, labels, epsilon=epsilon)\n",
    "                \n",
    "                # Calculate perturbation norm\n",
    "                perturbation = (adv_images - images).view(images.size(0), -1)\n",
    "                l2_norm = torch.norm(perturbation, p=2, dim=1).mean().item()\n",
    "                linf_norm = torch.norm(perturbation, p=float('inf'), dim=1).mean().item()\n",
    "                perturbation_norms.append({'l2': l2_norm, 'linf': linf_norm})\n",
    "                \n",
    "                # Adversarial prediction\n",
    "                with torch.no_grad():\n",
    "                    adv_outputs = model(adv_images)\n",
    "                    adv_pred = adv_outputs.argmax(dim=1)\n",
    "                    correct_adv += (adv_pred == labels).sum().item()\n",
    "            else:\n",
    "                correct_adv = correct_clean\n",
    "                perturbation_norms.append({'l2': 0.0, 'linf': 0.0})\n",
    "            \n",
    "            total += labels.size(0)\n",
    "        \n",
    "        clean_acc = 100.0 * correct_clean / total\n",
    "        adv_acc = 100.0 * correct_adv / total\n",
    "        success_rate = 100.0 * (correct_clean - correct_adv) / correct_clean if correct_clean > 0 else 0\n",
    "        \n",
    "        avg_l2_norm = np.mean([p['l2'] for p in perturbation_norms])\n",
    "        avg_linf_norm = np.mean([p['linf'] for p in perturbation_norms])\n",
    "        \n",
    "        result = {\n",
    "            'epsilon': epsilon,\n",
    "            'clean_accuracy': clean_acc,\n",
    "            'adversarial_accuracy': adv_acc,\n",
    "            'attack_success_rate': success_rate,\n",
    "            'avg_l2_perturbation': avg_l2_norm,\n",
    "            'avg_linf_perturbation': avg_linf_norm\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Clean Acc: {clean_acc:.2f}%\")\n",
    "        print(f\"  Adversarial Acc: {adv_acc:.2f}%\")\n",
    "        print(f\"  Attack Success Rate: {success_rate:.2f}%\")\n",
    "        print(f\"  Avg L2 Perturbation: {avg_l2_norm:.4f}\")\n",
    "        print(f\"  Avg L∞ Perturbation: {avg_linf_norm:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# FGSM Attack Analysis\n",
    "print(\"\\nFGSM ATTACK ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Epsilon values to test\n",
    "epsilon_values = [0.0, 0.01, 0.03, 0.05, 0.1, 0.2, 0.3]\n",
    "\n",
    "fgsm_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nAnalyzing FGSM attacks on {model_name}...\")\n",
    "    \n",
    "    # Create FGSM attack\n",
    "    fgsm_attack = FGSM(model, device=device)\n",
    "    \n",
    "    # Evaluate attack\n",
    "    results = evaluate_attack(\n",
    "        model, fgsm_attack, subset_loader, \n",
    "        f\"FGSM-{model_name}\", epsilon_values\n",
    "    )\n",
    "    \n",
    "    fgsm_results[model_name] = results\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "fgsm_df_data = []\n",
    "for model_name, results in fgsm_results.items():\n",
    "    for result in results:\n",
    "        result['model'] = model_name\n",
    "        result['attack'] = 'FGSM'\n",
    "        fgsm_df_data.append(result)\n",
    "\n",
    "fgsm_df = pd.DataFrame(fgsm_df_data)\n",
    "print(\"\\nFGSM attack evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM Attack Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attack_results(df, attack_name):\n",
    "    \"\"\"Visualize attack effectiveness across models\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'{attack_name} Attack Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models = df['model'].unique()\n",
    "    colors = ['blue', 'red', 'green', 'orange']\n",
    "    \n",
    "    # Plot 1: Adversarial Accuracy vs Epsilon\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df['model'] == model]\n",
    "        axes[0, 0].plot(model_data['epsilon'], model_data['adversarial_accuracy'], \n",
    "                       'o-', color=colors[i], label=model, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[0, 0].set_title('Adversarial Accuracy vs Epsilon', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epsilon')\n",
    "    axes[0, 0].set_ylabel('Adversarial Accuracy (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Attack Success Rate vs Epsilon\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df['model'] == model]\n",
    "        axes[0, 1].plot(model_data['epsilon'], model_data['attack_success_rate'], \n",
    "                       's-', color=colors[i], label=model, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[0, 1].set_title('Attack Success Rate vs Epsilon', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epsilon')\n",
    "    axes[0, 1].set_ylabel('Attack Success Rate (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: L2 Perturbation vs Epsilon\n",
    "    for i, model in enumerate(models):\n",
    "        model_data = df[df['model'] == model]\n",
    "        axes[1, 0].plot(model_data['epsilon'], model_data['avg_l2_perturbation'], \n",
    "                       '^-', color=colors[i], label=model, linewidth=2, markersize=6)\n",
    "    \n",
    "    axes[1, 0].set_title('L2 Perturbation Norm vs Epsilon', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epsilon')\n",
    "    axes[1, 0].set_ylabel('Average L2 Perturbation')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Model Robustness Comparison\n",
    "    epsilon_threshold = 0.1  # Compare robustness at eps=0.1\n",
    "    robustness_data = df[df['epsilon'] == epsilon_threshold]\n",
    "    \n",
    "    if not robustness_data.empty:\n",
    "        models_rob = robustness_data['model']\n",
    "        adv_accs = robustness_data['adversarial_accuracy']\n",
    "        \n",
    "        bars = axes[1, 1].bar(range(len(models_rob)), adv_accs, \n",
    "                             color=colors[:len(models_rob)], alpha=0.7)\n",
    "        axes[1, 1].set_title(f'Model Robustness (ε={epsilon_threshold})', fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Models')\n",
    "        axes[1, 1].set_ylabel('Adversarial Accuracy (%)')\n",
    "        axes[1, 1].set_xticks(range(len(models_rob)))\n",
    "        axes[1, 1].set_xticklabels(models_rob, rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                           f'{height:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_adversarial_examples(model, attack_method, dataloader, class_names, \n",
    "                                  epsilon_values=[0.05, 0.1, 0.2], num_samples=3):\n",
    "    \"\"\"Visualize adversarial examples\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of data\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    # Select samples to visualize\n",
    "    sample_indices = torch.randperm(images.size(0))[:num_samples]\n",
    "    sample_images = images[sample_indices]\n",
    "    sample_labels = labels[sample_indices]\n",
    "    \n",
    "    # Create figure\n",
    "    num_eps = len(epsilon_values)\n",
    "    fig, axes = plt.subplots(num_samples, num_eps + 1, figsize=(4 * (num_eps + 1), 4 * num_samples))\n",
    "    \n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    # Denormalization for display\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        original = sample_images[i:i+1]\n",
    "        true_label = sample_labels[i].item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            orig_output = model(original)\n",
    "            orig_pred = orig_output.argmax(dim=1).item()\n",
    "            orig_conf = F.softmax(orig_output, dim=1).max().item()\n",
    "        \n",
    "        # Display original\n",
    "        img_display = (original.squeeze() * std + mean).clamp(0, 1)\n",
    "        img_display = img_display.permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        axes[i, 0].imshow(img_display)\n",
    "        axes[i, 0].set_title(f'Original\\nTrue: {class_names[true_label]}\\n' + \n",
    "                            f'Pred: {class_names[orig_pred]}\\nConf: {orig_conf:.3f}', \n",
    "                            fontsize=10)\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Adversarial examples for different epsilons\n",
    "        for j, epsilon in enumerate(epsilon_values):\n",
    "            adv_image = attack_method.attack(original, sample_labels[i:i+1], epsilon=epsilon)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                adv_output = model(adv_image)\n",
    "                adv_pred = adv_output.argmax(dim=1).item()\n",
    "                adv_conf = F.softmax(adv_output, dim=1).max().item()\n",
    "            \n",
    "            # Calculate perturbation\n",
    "            perturbation = adv_image - original\n",
    "            l2_norm = torch.norm(perturbation.view(-1), p=2).item()\n",
    "            linf_norm = torch.norm(perturbation.view(-1), p=float('inf')).item()\n",
    "            \n",
    "            # Display adversarial image\n",
    "            adv_display = (adv_image.squeeze() * std + mean).clamp(0, 1)\n",
    "            adv_display = adv_display.permute(1, 2, 0).cpu().numpy()\n",
    "            \n",
    "            axes[i, j+1].imshow(adv_display)\n",
    "            \n",
    "            success = '✓' if adv_pred != true_label else '✗'\n",
    "            axes[i, j+1].set_title(f'ε={epsilon}\\nPred: {class_names[adv_pred]} {success}\\n' + \n",
    "                                  f'Conf: {adv_conf:.3f}\\nL2: {l2_norm:.3f}', \n",
    "                                  fontsize=10)\n",
    "            axes[i, j+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize FGSM results\n",
    "visualize_attack_results(fgsm_df, \"FGSM\")\n",
    "\n",
    "# Visualize adversarial examples for SimpleCNN\n",
    "if 'SimpleCNN' in models:\n",
    "    print(\"\\nVisualizing FGSM adversarial examples on SimpleCNN...\")\n",
    "    fgsm_attack = FGSM(models['SimpleCNN'], device=device)\n",
    "    visualize_adversarial_examples(\n",
    "        models['SimpleCNN'], fgsm_attack, subset_loader, class_names\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PGD Attack Implementation and Analysis\n",
    "\n",
    "Demonstrate the Projected Gradient Descent (PGD) attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PGD ATTACK ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# PGD configuration\n",
    "pgd_epsilon_values = [0.0, 0.03, 0.05, 0.1, 0.2]\n",
    "pgd_steps = 10\n",
    "pgd_step_size = 0.01\n",
    "\n",
    "pgd_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nAnalyzing PGD attacks on {model_name}...\")\n",
    "    \n",
    "    # Create PGD attack\n",
    "    pgd_attack = PGD(model, device=device, steps=pgd_steps, step_size=pgd_step_size)\n",
    "    \n",
    "    # Evaluate attack\n",
    "    results = evaluate_attack(\n",
    "        model, pgd_attack, subset_loader, \n",
    "        f\"PGD-{model_name}\", pgd_epsilon_values\n",
    "    )\n",
    "    \n",
    "    pgd_results[model_name] = results\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "pgd_df_data = []\n",
    "for model_name, results in pgd_results.items():\n",
    "    for result in results:\n",
    "        result['model'] = model_name\n",
    "        result['attack'] = 'PGD'\n",
    "        pgd_df_data.append(result)\n",
    "\n",
    "pgd_df = pd.DataFrame(pgd_df_data)\n",
    "\n",
    "# Visualize PGD results\n",
    "visualize_attack_results(pgd_df, \"PGD\")\n",
    "\n",
    "# Visualize PGD adversarial examples\n",
    "if 'SimpleCNN' in models:\n",
    "    print(\"\\nVisualizing PGD adversarial examples on SimpleCNN...\")\n",
    "    pgd_attack = PGD(models['SimpleCNN'], device=device, steps=pgd_steps, step_size=pgd_step_size)\n",
    "    visualize_adversarial_examples(\n",
    "        models['SimpleCNN'], pgd_attack, subset_loader, class_names,\n",
    "        epsilon_values=[0.05, 0.1, 0.2]\n",
    "    )\n",
    "\n",
    "print(\"\\nPGD attack evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Comparison and Analysis\n",
    "\n",
    "Compare FGSM and PGD attacks side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attacks(fgsm_df, pgd_df):\n",
    "    \"\"\"Compare different attack methods\"\"\"\n",
    "    \n",
    "    # Combine dataframes\n",
    "    combined_df = pd.concat([fgsm_df, pgd_df], ignore_index=True)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('FGSM vs PGD Attack Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    models = combined_df['model'].unique()\n",
    "    attacks = combined_df['attack'].unique()\n",
    "    \n",
    "    # Common epsilon values for fair comparison\n",
    "    common_epsilons = [0.03, 0.05, 0.1, 0.2]\n",
    "    \n",
    "    # Plot 1: Attack Success Rate Comparison\n",
    "    for model in models:\n",
    "        for attack in attacks:\n",
    "            data = combined_df[(combined_df['model'] == model) & \n",
    "                             (combined_df['attack'] == attack) & \n",
    "                             (combined_df['epsilon'].isin(common_epsilons))]\n",
    "            \n",
    "            if not data.empty:\n",
    "                linestyle = '-' if attack == 'FGSM' else '--'\n",
    "                marker = 'o' if model == 'SimpleCNN' else 's'\n",
    "                label = f'{model}-{attack}'\n",
    "                \n",
    "                axes[0, 0].plot(data['epsilon'], data['attack_success_rate'], \n",
    "                               linestyle=linestyle, marker=marker, label=label, linewidth=2)\n",
    "    \n",
    "    axes[0, 0].set_title('Attack Success Rate Comparison', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epsilon')\n",
    "    axes[0, 0].set_ylabel('Attack Success Rate (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Adversarial Accuracy Comparison\n",
    "    for model in models:\n",
    "        for attack in attacks:\n",
    "            data = combined_df[(combined_df['model'] == model) & \n",
    "                             (combined_df['attack'] == attack) & \n",
    "                             (combined_df['epsilon'].isin(common_epsilons))]\n",
    "            \n",
    "            if not data.empty:\n",
    "                linestyle = '-' if attack == 'FGSM' else '--'\n",
    "                marker = 'o' if model == 'SimpleCNN' else 's'\n",
    "                label = f'{model}-{attack}'\n",
    "                \n",
    "                axes[0, 1].plot(data['epsilon'], data['adversarial_accuracy'], \n",
    "                               linestyle=linestyle, marker=marker, label=label, linewidth=2)\n",
    "    \n",
    "    axes[0, 1].set_title('Adversarial Accuracy Comparison', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epsilon')\n",
    "    axes[0, 1].set_ylabel('Adversarial Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Model Robustness at fixed epsilon\n",
    "    epsilon_compare = 0.1\n",
    "    comparison_data = combined_df[combined_df['epsilon'] == epsilon_compare]\n",
    "    \n",
    "    if not comparison_data.empty:\n",
    "        x_pos = np.arange(len(models))\n",
    "        bar_width = 0.35\n",
    "        \n",
    "        for i, attack in enumerate(attacks):\n",
    "            attack_data = comparison_data[comparison_data['attack'] == attack]\n",
    "            values = []\n",
    "            for model in models:\n",
    "                model_data = attack_data[attack_data['model'] == model]\n",
    "                if not model_data.empty:\n",
    "                    values.append(model_data['adversarial_accuracy'].iloc[0])\n",
    "                else:\n",
    "                    values.append(0)\n",
    "            \n",
    "            axes[1, 0].bar(x_pos + i * bar_width, values, bar_width, \n",
    "                          label=attack, alpha=0.8)\n",
    "        \n",
    "        axes[1, 0].set_title(f'Model Robustness Comparison (ε={epsilon_compare})', fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Models')\n",
    "        axes[1, 0].set_ylabel('Adversarial Accuracy (%)')\n",
    "        axes[1, 0].set_xticks(x_pos + bar_width / 2)\n",
    "        axes[1, 0].set_xticklabels(models)\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Perturbation Analysis\n",
    "    for model in models:\n",
    "        for attack in attacks:\n",
    "            data = combined_df[(combined_df['model'] == model) & \n",
    "                             (combined_df['attack'] == attack) & \n",
    "                             (combined_df['epsilon'].isin(common_epsilons))]\n",
    "            \n",
    "            if not data.empty:\n",
    "                linestyle = '-' if attack == 'FGSM' else '--'\n",
    "                marker = 'o' if model == 'SimpleCNN' else 's'\n",
    "                label = f'{model}-{attack}'\n",
    "                \n",
    "                axes[1, 1].plot(data['epsilon'], data['avg_l2_perturbation'], \n",
    "                               linestyle=linestyle, marker=marker, label=label, linewidth=2)\n",
    "    \n",
    "    axes[1, 1].set_title('L2 Perturbation Comparison', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epsilon')\n",
    "    axes[1, 1].set_ylabel('Average L2 Perturbation')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Compare attacks\n",
    "print(\"\\nCOMPARING FGSM AND PGD ATTACKS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "combined_results = compare_attacks(fgsm_df, pgd_df)\n",
    "\n",
    "# Statistical analysis\n",
    "print(\"\\nAttack Effectiveness Analysis:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for epsilon in [0.05, 0.1, 0.2]:\n",
    "    print(f\"\\nAt ε = {epsilon}:\")\n",
    "    \n",
    "    epsilon_data = combined_results[combined_results['epsilon'] == epsilon]\n",
    "    \n",
    "    for model in models:\n",
    "        model_data = epsilon_data[epsilon_data['model'] == model]\n",
    "        \n",
    "        if not model_data.empty:\n",
    "            fgsm_data = model_data[model_data['attack'] == 'FGSM']\n",
    "            pgd_data = model_data[model_data['attack'] == 'PGD']\n",
    "            \n",
    "            if not fgsm_data.empty and not pgd_data.empty:\n",
    "                fgsm_success = fgsm_data['attack_success_rate'].iloc[0]\n",
    "                pgd_success = pgd_data['attack_success_rate'].iloc[0]\n",
    "                \n",
    "                print(f\"  {model}:\")\n",
    "                print(f\"    FGSM Success Rate: {fgsm_success:.2f}%\")\n",
    "                print(f\"    PGD Success Rate: {pgd_success:.2f}%\")\n",
    "                print(f\"    PGD Improvement: {pgd_success - fgsm_success:+.2f}%\")\n",
    "\n",
    "print(\"\\nAttack comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transferability Analysis\n",
    "\n",
    "Analyze the transferability of adversarial examples between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRANSFERABILITY ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if len(models) >= 2:\n",
    "    # Create transferability analyzer\n",
    "    transferability_analyzer = TransferabilityAnalyzer(\n",
    "        models=models, \n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Test transferability with both FGSM and PGD\n",
    "    attack_configs = {\n",
    "        'FGSM': {'method': 'fgsm', 'epsilon': 0.1},\n",
    "        'PGD': {'method': 'pgd', 'epsilon': 0.1, 'steps': 10, 'step_size': 0.01}\n",
    "    }\n",
    "    \n",
    "    transferability_results = {}\n",
    "    \n",
    "    for attack_name, attack_config in attack_configs.items():\n",
    "        print(f\"\\nAnalyzing {attack_name} transferability...\")\n",
    "        \n",
    "        # Run transferability analysis\n",
    "        transfer_results = transferability_analyzer.analyze_transferability(\n",
    "            dataloader=subset_loader,\n",
    "            attack_config=attack_config,\n",
    "            num_samples=50  # Smaller sample for faster computation\n",
    "        )\n",
    "        \n",
    "        transferability_results[attack_name] = transfer_results\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\n{attack_name} Transferability Matrix:\")\n",
    "        print(\"Source → Target | Success Rate\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for source_model, target_results in transfer_results.items():\n",
    "            for target_model, metrics in target_results.items():\n",
    "                if source_model != target_model:\n",
    "                    success_rate = metrics['attack_success_rate']\n",
    "                    print(f\"{source_model} → {target_model}: {success_rate:.2f}%\")\n",
    "    \n",
    "    # Visualize transferability\n",
    "    def visualize_transferability(transfer_results, attack_name):\n",
    "        \"\"\"Visualize transferability matrix\"\"\"\n",
    "        \n",
    "        model_names = list(transfer_results.keys())\n",
    "        n_models = len(model_names)\n",
    "        \n",
    "        # Create transferability matrix\n",
    "        transfer_matrix = np.zeros((n_models, n_models))\n",
    "        \n",
    "        for i, source in enumerate(model_names):\n",
    "            for j, target in enumerate(model_names):\n",
    "                if source == target:\n",
    "                    # Native attack success rate\n",
    "                    transfer_matrix[i, j] = transfer_results[source][target]['attack_success_rate']\n",
    "                else:\n",
    "                    # Cross-model transfer success rate\n",
    "                    transfer_matrix[i, j] = transfer_results[source][target]['attack_success_rate']\n",
    "        \n",
    "        # Plot heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(transfer_matrix, annot=True, fmt='.1f', cmap='Reds',\n",
    "                   xticklabels=[f'Target: {m}' for m in model_names],\n",
    "                   yticklabels=[f'Source: {m}' for m in model_names],\n",
    "                   cbar_kws={'label': 'Attack Success Rate (%)'})\n",
    "        \n",
    "        plt.title(f'{attack_name} Transferability Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Target Model')\n",
    "        plt.ylabel('Source Model')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return transfer_matrix\n",
    "    \n",
    "    # Visualize transferability for each attack\n",
    "    for attack_name, transfer_results in transferability_results.items():\n",
    "        print(f\"\\nVisualizing {attack_name} transferability...\")\n",
    "        matrix = visualize_transferability(transfer_results, attack_name)\n",
    "    \n",
    "    # Analyze transferability patterns\n",
    "    print(\"\\nTransferability Analysis Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for attack_name, transfer_results in transferability_results.items():\n",
    "        print(f\"\\n{attack_name} Attack:\")\n",
    "        \n",
    "        cross_transfer_rates = []\n",
    "        native_rates = []\n",
    "        \n",
    "        for source_model, target_results in transfer_results.items():\n",
    "            for target_model, metrics in target_results.items():\n",
    "                success_rate = metrics['attack_success_rate']\n",
    "                \n",
    "                if source_model == target_model:\n",
    "                    native_rates.append(success_rate)\n",
    "                else:\n",
    "                    cross_transfer_rates.append(success_rate)\n",
    "        \n",
    "        if cross_transfer_rates and native_rates:\n",
    "            avg_cross_transfer = np.mean(cross_transfer_rates)\n",
    "            avg_native = np.mean(native_rates)\n",
    "            \n",
    "            print(f\"  Average native attack success: {avg_native:.2f}%\")\n",
    "            print(f\"  Average cross-model transfer: {avg_cross_transfer:.2f}%\")\n",
    "            print(f\"  Transfer efficiency: {avg_cross_transfer/avg_native:.2f}\")\n",
    "            \n",
    "            if avg_cross_transfer > 50:\n",
    "                print(f\"  ✅ High transferability - attacks transfer well between models\")\n",
    "            elif avg_cross_transfer > 20:\n",
    "                print(f\"  ⚠️  Moderate transferability\")\n",
    "            else:\n",
    "                print(f\"  ❌ Low transferability\")\n",
    "\nelse:\n",
    "    print(\"\\n⚠️  Need at least 2 models for transferability analysis\")\n",
    "    print(\"Skipping transferability analysis.\")\n",
    "\n",
    "print(\"\\nTransferability analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Adversarial Interpretability Analysis\n",
    "\n",
    "Analyze how adversarial attacks affect model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ADVERSARIAL INTERPRETABILITY ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Select a model for interpretability analysis\n",
    "analysis_model_name = 'SimpleCNN' if 'SimpleCNN' in models else list(models.keys())[0]\n",
    "analysis_model = models[analysis_model_name]\n",
    "\n",
    "print(f\"Analyzing interpretability for: {analysis_model_name}\")\n",
    "\n",
    "# Create interpretability analyzer\n",
    "interp_analyzer = AdversarialInterpretabilityAnalyzer(\n",
    "    model=analysis_model,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get a few samples for detailed analysis\n",
    "sample_images, sample_labels = next(iter(subset_loader))\n",
    "sample_images = sample_images[:3].to(device)  # Analyze first 3 images\n",
    "sample_labels = sample_labels[:3].to(device)\n",
    "\n",
    "# Attack configurations for interpretability\n",
    "attack_configs = {\n",
    "    'FGSM_mild': {'method': 'fgsm', 'epsilon': 0.05},\n",
    "    'FGSM_strong': {'method': 'fgsm', 'epsilon': 0.15},\n",
    "    'PGD_mild': {'method': 'pgd', 'epsilon': 0.05, 'steps': 10, 'step_size': 0.005},\n",
    "    'PGD_strong': {'method': 'pgd', 'epsilon': 0.15, 'steps': 10, 'step_size': 0.015}\n",
    "}\n",
    "\n",
    "# Analyze interpretability changes\n",
    "print(\"\\nAnalyzing saliency map changes under adversarial attacks...\")\n",
    "\n",
    "interpretability_results = interp_analyzer.analyze_saliency_changes(\n",
    "    images=sample_images,\n",
    "    labels=sample_labels,\n",
    "    attack_configs=attack_configs,\n",
    "    class_names=class_names\n",
    ")\n",
    "\n",
    "print(\"Interpretability analysis complete!\")\n",
    "\n",
    "# Create custom visualization for interpretability\n",
    "def visualize_interpretability_changes(images, labels, results, class_names):\n",
    "    \"\"\"Visualize how adversarial attacks affect model interpretability\"\"\"\n",
    "    \n",
    "    n_samples = min(2, images.size(0))  # Limit to 2 samples for readability\n",
    "    n_attacks = len(attack_configs) + 1  # +1 for original\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples * 2, n_attacks, figsize=(4 * n_attacks, 8 * n_samples))\n",
    "    \n",
    "    if n_samples == 1:\n",
    "        axes = axes.reshape(2, -1)\n",
    "    \n",
    "    # Denormalization\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        true_label = labels[sample_idx].item()\n",
    "        \n",
    "        # Original image and saliency\n",
    "        original_img = images[sample_idx:sample_idx+1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            orig_output = analysis_model(original_img)\n",
    "            orig_pred = orig_output.argmax(dim=1).item()\n",
    "            orig_conf = F.softmax(orig_output, dim=1).max().item()\n",
    "        \n",
    "        # Display original image\n",
    "        img_display = (original_img.squeeze() * std + mean).clamp(0, 1)\n",
    "        img_display = img_display.permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        axes[sample_idx*2, 0].imshow(img_display)\n",
    "        axes[sample_idx*2, 0].set_title(f'Original\\nTrue: {class_names[true_label]}\\n' + \n",
    "                                       f'Pred: {class_names[orig_pred]}\\nConf: {orig_conf:.3f}', \n",
    "                                       fontsize=10)\n",
    "        axes[sample_idx*2, 0].axis('off')\n",
    "        \n",
    "        # Original saliency\n",
    "        saliency_map = SaliencyMap(analysis_model, device=device)\n",
    "        orig_saliency, _, _ = saliency_map.generate(original_img)\n",
    "        \n",
    "        axes[sample_idx*2+1, 0].imshow(orig_saliency, cmap='hot')\n",
    "        axes[sample_idx*2+1, 0].set_title('Original Saliency', fontsize=10)\n",
    "        axes[sample_idx*2+1, 0].axis('off')\n",
    "        \n",
    "        # Adversarial examples and their saliencies\n",
    "        for col_idx, (attack_name, config) in enumerate(attack_configs.items(), 1):\n",
    "            if attack_name in results and sample_idx < len(results[attack_name]['adversarial_images']):\n",
    "                adv_img = results[attack_name]['adversarial_images'][sample_idx]\n",
    "                adv_saliency = results[attack_name]['adversarial_saliencies'][sample_idx]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    adv_output = analysis_model(adv_img.unsqueeze(0))\n",
    "                    adv_pred = adv_output.argmax(dim=1).item()\n",
    "                    adv_conf = F.softmax(adv_output, dim=1).max().item()\n",
    "                \n",
    "                # Display adversarial image\n",
    "                adv_display = (adv_img * std.squeeze() + mean.squeeze()).clamp(0, 1)\n",
    "                adv_display = adv_display.permute(1, 2, 0).cpu().numpy()\n",
    "                \n",
    "                success = '✓' if adv_pred != true_label else '✗'\n",
    "                axes[sample_idx*2, col_idx].imshow(adv_display)\n",
    "                axes[sample_idx*2, col_idx].set_title(\n",
    "                    f'{attack_name}\\nPred: {class_names[adv_pred]} {success}\\nConf: {adv_conf:.3f}',\n",
    "                    fontsize=10\n",
    "                )\n",
    "                axes[sample_idx*2, col_idx].axis('off')\n",
    "                \n",
    "                # Display adversarial saliency\n",
    "                axes[sample_idx*2+1, col_idx].imshow(adv_saliency, cmap='hot')\n",
    "                \n",
    "                # Calculate saliency difference\n",
    "                saliency_diff = np.abs(adv_saliency - orig_saliency).mean()\n",
    "                axes[sample_idx*2+1, col_idx].set_title(\n",
    "                    f'{attack_name} Saliency\\nDiff: {saliency_diff:.3f}',\n",
    "                    fontsize=10\n",
    "                )\n",
    "                axes[sample_idx*2+1, col_idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize interpretability changes\n",
    "if interpretability_results:\n",
    "    print(\"\\nVisualizing interpretability changes...\")\n",
    "    visualize_interpretability_changes(\n",
    "        sample_images, sample_labels, interpretability_results, class_names\n",
    "    )\n",
    "\n",
    "# Quantitative analysis of interpretability changes\n",
    "print(\"\\nQuantitative Interpretability Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "if interpretability_results:\n",
    "    for attack_name, attack_results in interpretability_results.items():\n",
    "        if 'saliency_differences' in attack_results:\n",
    "            saliency_diffs = attack_results['saliency_differences']\n",
    "            avg_diff = np.mean(saliency_diffs)\n",
    "            std_diff = np.std(saliency_diffs)\n",
    "            \n",
    "            print(f\"\\n{attack_name}:\")\n",
    "            print(f\"  Average saliency change: {avg_diff:.4f} ± {std_diff:.4f}\")\n",
    "            print(f\"  Max saliency change: {np.max(saliency_diffs):.4f}\")\n",
    "            print(f\"  Min saliency change: {np.min(saliency_diffs):.4f}\")\n",
    "            \n",
    "            if avg_diff > 0.1:\n",
    "                print(f\"  ⚠️  High interpretability change\")\n",
    "            elif avg_diff > 0.05:\n",
    "                print(f\"  ⚠️  Moderate interpretability change\")\n",
    "            else:\n",
    "                print(f\"  ✅ Low interpretability change\")\n",
    "\n",
    "print(\"\\nInterpretability analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Robustness Evaluation and Defense Strategies\n",
    "\n",
    "Evaluate model robustness and discuss potential defense strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODEL ROBUSTNESS EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def comprehensive_robustness_evaluation(models, dataloader, class_names):\n",
    "    \"\"\"Comprehensive evaluation of model robustness\"\"\"\n",
    "    \n",
    "    robustness_results = {}\n",
    "    \n",
    "    # Test multiple attack configurations\n",
    "    test_configs = {\n",
    "        'weak_fgsm': {'attack': 'fgsm', 'epsilon': 0.03},\n",
    "        'medium_fgsm': {'attack': 'fgsm', 'epsilon': 0.1},\n",
    "        'strong_fgsm': {'attack': 'fgsm', 'epsilon': 0.2},\n",
    "        'weak_pgd': {'attack': 'pgd', 'epsilon': 0.03, 'steps': 10, 'step_size': 0.003},\n",
    "        'medium_pgd': {'attack': 'pgd', 'epsilon': 0.1, 'steps': 10, 'step_size': 0.01},\n",
    "        'strong_pgd': {'attack': 'pgd', 'epsilon': 0.2, 'steps': 10, 'step_size': 0.02}\n",
    "    }\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating robustness of {model_name}...\")\n",
    "        \n",
    "        model_results = {}\n",
    "        \n",
    "        # Clean accuracy\n",
    "        clean_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                predictions = outputs.argmax(dim=1)\n",
    "                clean_correct += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "        \n",
    "        clean_accuracy = 100.0 * clean_correct / total_samples\n",
    "        model_results['clean_accuracy'] = clean_accuracy\n",
    "        \n",
    "        print(f\"  Clean accuracy: {clean_accuracy:.2f}%\")\n",
    "        \n",
    "        # Test each attack configuration\n",
    "        for config_name, config in test_configs.items():\n",
    "            print(f\"    Testing {config_name}...\")\n",
    "            \n",
    "            if config['attack'] == 'fgsm':\n",
    "                attack_method = FGSM(model, device=device)\n",
    "                epsilon = config['epsilon']\n",
    "            else:  # pgd\n",
    "                attack_method = PGD(model, device=device, \n",
    "                                  steps=config['steps'], \n",
    "                                  step_size=config['step_size'])\n",
    "                epsilon = config['epsilon']\n",
    "            \n",
    "            adv_correct = 0\n",
    "            \n",
    "            for images, labels in dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Generate adversarial examples\n",
    "                adv_images = attack_method.attack(images, labels, epsilon=epsilon)\n",
    "                \n",
    "                # Test adversarial accuracy\n",
    "                with torch.no_grad():\n",
    "                    adv_outputs = model(adv_images)\n",
    "                    adv_predictions = adv_outputs.argmax(dim=1)\n",
    "                    adv_correct += (adv_predictions == labels).sum().item()\n",
    "            \n",
    "            adv_accuracy = 100.0 * adv_correct / total_samples\n",
    "            robustness_score = adv_accuracy / clean_accuracy if clean_accuracy > 0 else 0\n",
    "            \n",
    "            model_results[config_name] = {\n",
    "                'adversarial_accuracy': adv_accuracy,\n",
    "                'robustness_score': robustness_score,\n",
    "                'attack_success_rate': 100.0 * (clean_correct - adv_correct) / clean_correct if clean_correct > 0 else 0\n",
    "            }\n",
    "            \n",
    "            print(f\"      Adv accuracy: {adv_accuracy:.2f}%\")\n",
    "            print(f\"      Robustness score: {robustness_score:.3f}\")\n",
    "        \n",
    "        robustness_results[model_name] = model_results\n",
    "    \n",
    "    return robustness_results\n",
    "\n",
    "# Run comprehensive robustness evaluation\n",
    "robustness_results = comprehensive_robustness_evaluation(\n",
    "    models, subset_loader, class_names\n",
    ")\n",
    "\n",
    "# Visualize robustness comparison\n",
    "def visualize_robustness_comparison(robustness_results):\n",
    "    \"\"\"Visualize robustness comparison across models and attacks\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Model Robustness Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    model_names = list(robustness_results.keys())\n",
    "    attack_configs = ['weak_fgsm', 'medium_fgsm', 'strong_fgsm', \n",
    "                     'weak_pgd', 'medium_pgd', 'strong_pgd']\n",
    "    \n",
    "    # Plot 1: Robustness scores heatmap\n",
    "    robustness_matrix = []\n",
    "    for model_name in model_names:\n",
    "        model_scores = []\n",
    "        for attack_config in attack_configs:\n",
    "            if attack_config in robustness_results[model_name]:\n",
    "                score = robustness_results[model_name][attack_config]['robustness_score']\n",
    "                model_scores.append(score)\n",
    "            else:\n",
    "                model_scores.append(0)\n",
    "        robustness_matrix.append(model_scores)\n",
    "    \n",
    "    robustness_matrix = np.array(robustness_matrix)\n",
    "    \n",
    "    im = axes[0, 0].imshow(robustness_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[0, 0].set_title('Robustness Score Matrix', fontweight='bold')\n",
    "    axes[0, 0].set_xticks(range(len(attack_configs)))\n",
    "    axes[0, 0].set_xticklabels(attack_configs, rotation=45, ha='right')\n",
    "    axes[0, 0].set_yticks(range(len(model_names)))\n",
    "    axes[0, 0].set_yticklabels(model_names)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(len(attack_configs)):\n",
    "            axes[0, 0].text(j, i, f'{robustness_matrix[i, j]:.2f}', \n",
    "                           ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[0, 0], label='Robustness Score')\n",
    "    \n",
    "    # Plot 2: Average robustness by model\n",
    "    avg_robustness = []\n",
    "    for model_name in model_names:\n",
    "        scores = [robustness_results[model_name][config]['robustness_score'] \n",
    "                 for config in attack_configs \n",
    "                 if config in robustness_results[model_name]]\n",
    "        avg_robustness.append(np.mean(scores))\n",
    "    \n",
    "    bars = axes[0, 1].bar(model_names, avg_robustness, alpha=0.7)\n",
    "    axes[0, 1].set_title('Average Robustness Score', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Average Robustness Score')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{avg_robustness[i]:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 3: Attack success rates\n",
    "    fgsm_attacks = ['weak_fgsm', 'medium_fgsm', 'strong_fgsm']\n",
    "    pgd_attacks = ['weak_pgd', 'medium_pgd', 'strong_pgd']\n",
    "    \n",
    "    x_pos = np.arange(len(model_names))\n",
    "    bar_width = 0.35\n",
    "    \n",
    "    fgsm_success = []\n",
    "    pgd_success = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        fgsm_rates = [robustness_results[model_name][attack]['attack_success_rate']\n",
    "                     for attack in fgsm_attacks \n",
    "                     if attack in robustness_results[model_name]]\n",
    "        pgd_rates = [robustness_results[model_name][attack]['attack_success_rate']\n",
    "                    for attack in pgd_attacks \n",
    "                    if attack in robustness_results[model_name]]\n",
    "        \n",
    "        fgsm_success.append(np.mean(fgsm_rates) if fgsm_rates else 0)\n",
    "        pgd_success.append(np.mean(pgd_rates) if pgd_rates else 0)\n",
    "    \n",
    "    axes[1, 0].bar(x_pos - bar_width/2, fgsm_success, bar_width, \n",
    "                  label='FGSM', alpha=0.8)\n",
    "    axes[1, 0].bar(x_pos + bar_width/2, pgd_success, bar_width, \n",
    "                  label='PGD', alpha=0.8)\n",
    "    \n",
    "    axes[1, 0].set_title('Average Attack Success Rate', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Models')\n",
    "    axes[1, 0].set_ylabel('Attack Success Rate (%)')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(model_names)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Robustness vs Clean Accuracy\n",
    "    clean_accs = [robustness_results[model]['clean_accuracy'] for model in model_names]\n",
    "    \n",
    "    axes[1, 1].scatter(clean_accs, avg_robustness, s=100, alpha=0.7)\n",
    "    \n",
    "    for i, model in enumerate(model_names):\n",
    "        axes[1, 1].annotate(model, (clean_accs[i], avg_robustness[i]), \n",
    "                           xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    axes[1, 1].set_title('Robustness vs Clean Accuracy', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Clean Accuracy (%)')\n",
    "    axes[1, 1].set_ylabel('Average Robustness Score')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize robustness comparison\n",
    "visualize_robustness_comparison(robustness_results)\n",
    "\n",
    "# Print robustness summary\n",
    "print(\"\\nROBUSTNESS EVALUATION SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name, results in robustness_results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Clean accuracy: {results['clean_accuracy']:.2f}%\")\n",
    "    \n",
    "    # Calculate average robustness\n",
    "    robustness_scores = [results[config]['robustness_score'] \n",
    "                        for config in results if config != 'clean_accuracy']\n",
    "    avg_robustness = np.mean(robustness_scores)\n",
    "    \n",
    "    print(f\"  Average robustness score: {avg_robustness:.3f}\")\n",
    "    \n",
    "    if avg_robustness > 0.7:\n",
    "        print(f\"  ✅ High robustness\")\n",
    "    elif avg_robustness > 0.4:\n",
    "        print(f\"  ⚠️  Moderate robustness\")\n",
    "    else:\n",
    "        print(f\"  ❌ Low robustness\")\n",
    "\n",
    "print(\"\\nRobustness evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions\n",
    "\n",
    "Summarize findings and provide recommendations for improving adversarial robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVERSARIAL ATTACKS ANALYSIS - SUMMARY AND CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Key findings from the analysis\n",
    "print(\"\\n🔍 KEY FINDINGS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# 1. Attack Effectiveness\n",
    "print(\"\\n1. Attack Effectiveness:\")\n",
    "if 'combined_results' in locals():\n",
    "    # Find most effective attack\n",
    "    high_eps_data = combined_results[combined_results['epsilon'] >= 0.1]\n",
    "    if not high_eps_data.empty:\n",
    "        best_attack = high_eps_data.loc[high_eps_data['attack_success_rate'].idxmax()]\n",
    "        print(f\"   • Most effective attack: {best_attack['attack']} on {best_attack['model']}\")\n",
    "        print(f\"   • Success rate: {best_attack['attack_success_rate']:.1f}% at ε={best_attack['epsilon']}\")\n",
    "        \n",
    "        # Compare FGSM vs PGD\n",
    "        fgsm_avg = combined_results[combined_results['attack'] == 'FGSM']['attack_success_rate'].mean()\n",
    "        pgd_avg = combined_results[combined_results['attack'] == 'PGD']['attack_success_rate'].mean()\n",
    "        \n",
    "        print(f\"   • Average FGSM success rate: {fgsm_avg:.1f}%\")\n",
    "        print(f\"   • Average PGD success rate: {pgd_avg:.1f}%\")\n",
    "        \n",
    "        if pgd_avg > fgsm_avg:\n",
    "            print(f\"   ✓ PGD is {pgd_avg - fgsm_avg:.1f}% more effective than FGSM\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ FGSM performs similarly to PGD\")\n",
    "\n",
    "# 2. Model Robustness\n",
    "print(\"\\n2. Model Robustness Ranking:\")\n",
    "if robustness_results:\n",
    "    # Calculate overall robustness scores\n",
    "    model_robustness = {}\n",
    "    for model_name, results in robustness_results.items():\n",
    "        scores = [results[config]['robustness_score'] \n",
    "                 for config in results if config != 'clean_accuracy']\n",
    "        model_robustness[model_name] = np.mean(scores)\n",
    "    \n",
    "    # Sort by robustness\n",
    "    sorted_models = sorted(model_robustness.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (model, score) in enumerate(sorted_models, 1):\n",
    "        clean_acc = robustness_results[model]['clean_accuracy']\n",
    "        print(f\"   {i}. {model}: {score:.3f} robustness (Clean: {clean_acc:.1f}%)\")\n",
    "    \n",
    "    # Best balance between accuracy and robustness\n",
    "    balance_scores = {}\n",
    "    for model_name, results in robustness_results.items():\n",
    "        clean_acc = results['clean_accuracy']\n",
    "        robust_score = model_robustness[model_name]\n",
    "        balance_scores[model_name] = (clean_acc / 100) * robust_score\n",
    "    \n",
    "    best_balance = max(balance_scores.items(), key=lambda x: x[1])\n",
    "    print(f\"   ✓ Best accuracy-robustness balance: {best_balance[0]} (score: {best_balance[1]:.3f})\")\n",
    "\n",
    "# 3. Transferability\n",
    "if 'transferability_results' in locals() and transferability_results:\n",
    "    print(\"\\n3. Transferability Insights:\")\n",
    "    \n",
    "    for attack_name, transfer_results in transferability_results.items():\n",
    "        cross_transfer_rates = []\n",
    "        \n",
    "        for source_model, target_results in transfer_results.items():\n",
    "            for target_model, metrics in target_results.items():\n",
    "                if source_model != target_model:\n",
    "                    cross_transfer_rates.append(metrics['attack_success_rate'])\n",
    "        \n",
    "        if cross_transfer_rates:\n",
    "            avg_transfer = np.mean(cross_transfer_rates)\n",
    "            print(f\"   • {attack_name} average transferability: {avg_transfer:.1f}%\")\n",
    "            \n",
    "            if avg_transfer > 50:\n",
    "                print(f\"     ⚠️ High transferability - models share vulnerabilities\")\n",
    "            else:\n",
    "                print(f\"     ✓ Limited transferability\")\n",
    "\n",
    "# 4. Interpretability Impact\n",
    "if 'interpretability_results' in locals() and interpretability_results:\n",
    "    print(\"\\n4. Interpretability Impact:\")\n",
    "    \n",
    "    for attack_name, attack_results in interpretability_results.items():\n",
    "        if 'saliency_differences' in attack_results:\n",
    "            avg_diff = np.mean(attack_results['saliency_differences'])\n",
    "            print(f\"   • {attack_name} saliency change: {avg_diff:.3f}\")\n",
    "            \n",
    "            if avg_diff > 0.1:\n",
    "                print(f\"     ⚠️ Significant interpretability disruption\")\n",
    "            else:\n",
    "                print(f\"     ✓ Moderate interpretability impact\")\n",
    "\n",
    "# Security Implications\n",
    "print(\"\\n🚨 SECURITY IMPLICATIONS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"\\n1. Attack Feasibility:\")\n",
    "print(\"   • FGSM attacks can be generated quickly with minimal computation\")\n",
    "print(\"   • PGD attacks are more powerful but require more computational resources\")\n",
    "print(\"   • Small perturbations (ε < 0.1) can significantly reduce accuracy\")\n",
    "print(\"   • Attacks can fool models while remaining visually imperceptible\")\n",
    "\n",
    "print(\"\\n2. Real-World Risks:\")\n",
    "print(\"   • Sports classification systems could misclassify images\")\n",
    "print(\"   • Potential for bypassing content filtering systems\")\n",
    "print(\"   • Risk in automated sports analysis and broadcasting\")\n",
    "\n",
    "if 'transferability_results' in locals() and transferability_results:\n",
    "    print(\"\\n3. Cross-Model Vulnerabilities:\")\n",
    "    if any(np.mean([transfer_results[source][target]['attack_success_rate'] \n",
    "                   for source in transfer_results \n",
    "                   for target in transfer_results[source] \n",
    "                   if source != target]) > 30 \n",
    "          for transfer_results in transferability_results.values()):\n",
    "        print(\"   ⚠️ Models share common vulnerabilities\")\n",
    "        print(\"   • Black-box attacks possible using surrogate models\")\n",
    "        print(\"   • Need for diverse training approaches\")\n",
    "    else:\n",
    "        print(\"   ✓ Limited cross-model vulnerability transfer\")\n",
    "\n",
    "# Defense Recommendations\n",
    "print(\"\\n🛡️  DEFENSE RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"\\n1. Adversarial Training:\")\n",
    "print(\"   • Train models with adversarial examples in the training set\")\n",
    "print(\"   • Use multiple attack types (FGSM, PGD, C&W) during training\")\n",
    "print(\"   • Gradually increase attack strength during training\")\n",
    "print(\"   • Expected improvement: 20-40% robustness increase\")\n",
    "\n",
    "print(\"\\n2. Data Augmentation:\")\n",
    "print(\"   • Add Gaussian noise during training\")\n",
    "print(\"   • Use random transformations beyond current augmentations\")\n",
    "print(\"   • Implement mixup and cutmix techniques\")\n",
    "print(\"   • Expected improvement: 10-20% robustness increase\")\n",
    "\n",
    "print(\"\\n3. Model Architecture Improvements:\")\n",
    "print(\"   • Use certified defense layers\")\n",
    "print(\"   • Implement defensive distillation\")\n",
    "print(\"   • Add batch normalization and dropout for regularization\")\n",
    "print(\"   • Consider ensemble methods for improved robustness\")\n",
    "\n",
    "print(\"\\n4. Input Preprocessing:\")\n",
    "print(\"   • Apply image denoising filters\")\n",
    "print(\"   • Use JPEG compression to remove small perturbations\")\n",
    "print(\"   • Implement randomized smoothing\")\n",
    "print(\"   • Note: May slightly reduce clean accuracy\")\n",
    "\n",
    "print(\"\\n5. Detection Mechanisms:\")\n",
    "print(\"   • Monitor prediction confidence scores\")\n",
    "print(\"   • Detect unusual activation patterns\")\n",
    "print(\"   • Use statistical tests on model outputs\")\n",
    "print(\"   • Implement uncertainty quantification\")\n",
    "\n",
    "# Future Work\n",
    "print(\"\\n🔬 FUTURE RESEARCH DIRECTIONS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "print(\"\\n1. Advanced Attacks:\")\n",
    "print(\"   • Semantic adversarial attacks\")\n",
    "print(\"   • Physical adversarial examples\")\n",
    "print(\"   • Adaptive attacks against specific defenses\")\n",
    "print(\"   • Universal adversarial perturbations\")\n",
    "\n",
    "print(\"\\n2. Robust Training Methods:\")\n",
    "print(\"   • Certified adversarial training\")\n",
    "print(\"   • Distributionally robust optimization\")\n",
    "print(\"   • Meta-learning for adversarial robustness\")\n",
    "print(\"   • Self-supervised robust pretraining\")\n",
    "\n",
    "print(\"\\n3. Evaluation Metrics:\")\n",
    "print(\"   • Develop better robustness metrics\")\n",
    "print(\"   • Study robustness-accuracy trade-offs\")\n",
    "print(\"   • Analyze robustness across different domains\")\n",
    "\n",
    "# Implementation Priority\n",
    "print(\"\\n📋 IMPLEMENTATION PRIORITY:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if robustness_results:\n",
    "    # Assess current robustness level\n",
    "    avg_robustness = np.mean([np.mean([results[config]['robustness_score'] \n",
    "                                     for config in results if config != 'clean_accuracy']) \n",
    "                            for results in robustness_results.values()])\n",
    "    \n",
    "    print(f\"\\nCurrent average robustness: {avg_robustness:.3f}\")\n",
    "    \n",
    "    if avg_robustness < 0.3:\n",
    "        print(\"\\n🚨 HIGH PRIORITY - Low robustness detected:\")\n",
    "        print(\"   1. Implement adversarial training immediately\")\n",
    "        print(\"   2. Add input preprocessing defenses\")\n",
    "        print(\"   3. Implement attack detection\")\n",
    "    elif avg_robustness < 0.6:\n",
    "        print(\"\\n⚠️ MEDIUM PRIORITY - Moderate robustness:\")\n",
    "        print(\"   1. Enhance data augmentation\")\n",
    "        print(\"   2. Experiment with adversarial training\")\n",
    "        print(\"   3. Consider ensemble methods\")\n",
    "    else:\n",
    "        print(\"\\n✅ LOW PRIORITY - Good robustness:\")\n",
    "        print(\"   1. Fine-tune existing defenses\")\n",
    "        print(\"   2. Test against stronger attacks\")\n",
    "        print(\"   3. Focus on maintaining clean accuracy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"📊 Analysis complete! Use these insights to improve model robustness.\")\n",
    "print(\"🔒 Remember: Security is an ongoing process, not a one-time fix.\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}